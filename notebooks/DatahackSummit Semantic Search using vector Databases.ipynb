{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e03b93",
   "metadata": {},
   "source": [
    "# Building Your Own Search Engine Using Vector Databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24e8918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:40:26.682054Z",
     "start_time": "2023-07-05T08:40:26.542951Z"
    }
   },
   "source": [
    "![img](https://www.analyticsvidhya.com/datahack-summit-2023/wp-content/uploads/2023/07/s-won_searchengin.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef2022",
   "metadata": {},
   "source": [
    "The dataset used is https://www.kaggle.com/datasets/benhamner/nips-papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64b282",
   "metadata": {},
   "source": [
    "## Libraries and Technologies we will use\n",
    "\n",
    "1) Pre Trained Large Language Model (LLM) like ChatGPT for vector Embedding\n",
    "2) Langchain for Supporting our model application\n",
    "3) Vector Database/index like Chroma, Elasticsearch\n",
    "4) Gradio\n",
    "\n",
    "\n",
    "## Generic Architecture\n",
    "\n",
    "![arch](https://ghost.hacksoft.io/content/images/2023/04/answering_questions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb888ac7",
   "metadata": {},
   "source": [
    "# Hands On Coding\n",
    "\n",
    "## Dependencies Installation and Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39fcfbe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:34.148258Z",
     "start_time": "2023-08-02T17:47:34.145137Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install langchain openai chromadb kaggle sentence_transformers datasets gradio elasticsearch tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3134955",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:34.158989Z",
     "start_time": "2023-08-02T17:47:34.153527Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "if not os.path.isfile(\"database.sqlite\"):\n",
    "    os.system(\"kaggle datasets download benhamner/nips-papersv\")\n",
    "    os.system(\"unzip -o nips-papers.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26bdddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:38.206315Z",
     "start_time": "2023-08-02T17:47:34.163776Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "import sqlite3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b4e167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:39.874250Z",
     "start_time": "2023-08-02T17:47:38.213470Z"
    }
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"database.sqlite\")\n",
    "\n",
    "sql= \"\"\"WITH paper_author_list AS (\n",
    "    SELECT papers.id AS paper_id, Group_concat(authors.name) AS author_list\n",
    "    FROM papers\n",
    "    JOIN paper_authors ON papers.id = paper_authors.paper_id\n",
    "    JOIN authors ON paper_authors.author_id = authors.id\n",
    "    GROUP BY paper_id\n",
    ")\n",
    "SELECT papers.id AS paper_id, papers.year, papers.title, papers.abstract, papers.paper_text, paper_author_list.author_list AS authors\n",
    "FROM papers\n",
    "JOIN paper_author_list ON papers.id = paper_author_list.paper_id\n",
    "WHERE abstract NOT LIKE '%Abstract Missing%'\n",
    "\n",
    "\"\"\";\n",
    "\n",
    "papers_df = pd.read_sql_query(sql, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481493fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:39.910022Z",
     "start_time": "2023-08-02T17:47:39.876583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1861</td>\n",
       "      <td>2000</td>\n",
       "      <td>Algorithms for Non-negative Matrix Factorization</td>\n",
       "      <td>Non-negative matrix factorization (NMF) has pr...</td>\n",
       "      <td>Algorithms for Non-negative Matrix\\nFactorizat...</td>\n",
       "      <td>Daniel D. Lee,H. Sebastian Seung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1975</td>\n",
       "      <td>2001</td>\n",
       "      <td>Characterizing Neural Gain Control using Spike...</td>\n",
       "      <td>Spike-triggered averaging techniques are effec...</td>\n",
       "      <td>Characterizing neural gain control using\\nspik...</td>\n",
       "      <td>Odelia Schwartz,E.J. Chichilnisky,Eero P. Simo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3163</td>\n",
       "      <td>2007</td>\n",
       "      <td>Competition Adds Complexity</td>\n",
       "      <td>It is known that determinining whether a DEC-P...</td>\n",
       "      <td>Competition adds complexity\\n\\nJudy Goldsmith\\...</td>\n",
       "      <td>Judy Goldsmith,Martin Mundhenk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3164</td>\n",
       "      <td>2007</td>\n",
       "      <td>Efficient Principled Learning of Thin Junction...</td>\n",
       "      <td>We present the first truly polynomial algorith...</td>\n",
       "      <td>Efficient Principled Learning of Thin Junction...</td>\n",
       "      <td>Anton Chechetka,Carlos Guestrin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3167</td>\n",
       "      <td>2007</td>\n",
       "      <td>Regularized Boost for Semi-Supervised Learning</td>\n",
       "      <td>Semi-supervised inductive learning concerns ho...</td>\n",
       "      <td>Regularized Boost for Semi-Supervised Learning...</td>\n",
       "      <td>Ke Chen,Shihai Wang</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  year                                              title  \\\n",
       "0      1861  2000   Algorithms for Non-negative Matrix Factorization   \n",
       "1      1975  2001  Characterizing Neural Gain Control using Spike...   \n",
       "2      3163  2007                        Competition Adds Complexity   \n",
       "3      3164  2007  Efficient Principled Learning of Thin Junction...   \n",
       "4      3167  2007     Regularized Boost for Semi-Supervised Learning   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Non-negative matrix factorization (NMF) has pr...   \n",
       "1  Spike-triggered averaging techniques are effec...   \n",
       "2  It is known that determinining whether a DEC-P...   \n",
       "3  We present the first truly polynomial algorith...   \n",
       "4  Semi-supervised inductive learning concerns ho...   \n",
       "\n",
       "                                          paper_text  \\\n",
       "0  Algorithms for Non-negative Matrix\\nFactorizat...   \n",
       "1  Characterizing neural gain control using\\nspik...   \n",
       "2  Competition adds complexity\\n\\nJudy Goldsmith\\...   \n",
       "3  Efficient Principled Learning of Thin Junction...   \n",
       "4  Regularized Boost for Semi-Supervised Learning...   \n",
       "\n",
       "                                             authors  \n",
       "0                   Daniel D. Lee,H. Sebastian Seung  \n",
       "1  Odelia Schwartz,E.J. Chichilnisky,Eero P. Simo...  \n",
       "2                     Judy Goldsmith,Martin Mundhenk  \n",
       "3                    Anton Chechetka,Carlos Guestrin  \n",
       "4                                Ke Chen,Shihai Wang  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6d5bf7e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:42:26.913820Z",
     "start_time": "2023-08-02T18:42:26.882532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3921 entries, 0 to 3920\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   paper_id  3921 non-null   int64 \n",
      " 1   year      3921 non-null   int64 \n",
      " 2   authors   3921 non-null   object\n",
      " 3   title     3921 non-null   object\n",
      " 4   abstract  3921 non-null   object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 153.3+ KB\n"
     ]
    }
   ],
   "source": [
    "papers_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58712470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:39.920778Z",
     "start_time": "2023-08-02T17:47:39.913451Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_text(papers_df, column):\n",
    "    \n",
    "    # Load the regular expression library\n",
    "    import re\n",
    "    preprocessed_column = f\"{column}_processed\"\n",
    "\n",
    "    # Print the titles of the first rows \n",
    "    print(papers_df[column].head())\n",
    "\n",
    "    # remove punctuations\n",
    "    #papers_df[preprocessed_column] = papers_df[column].map(lambda x: re.sub('[,!?]', '', x))\n",
    "    \n",
    "     # remove carriage return and end of line\n",
    "    papers_df[preprocessed_column] = papers_df[column].map(lambda x: re.sub('[\\r\\n]', ' ', x))\n",
    "    \n",
    "     # remove double spaces\n",
    "    papers_df[preprocessed_column] = papers_df[preprocessed_column].map(lambda x: re.sub('  ', ' ', x))\n",
    "\n",
    "    \n",
    "    # remove para continuation\n",
    "    papers_df[preprocessed_column] = papers_df[preprocessed_column].map(lambda x: re.sub('- ', '', x))\n",
    "\n",
    "    # Convert the titles to lowercase\n",
    "    papers_df[preprocessed_column] = papers_df[preprocessed_column].map(lambda x: x.lower())\n",
    "    return papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1362e65f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:42.680239Z",
     "start_time": "2023-08-02T17:47:39.923303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Algorithms for Non-negative Matrix Factorization\n",
      "1    Characterizing Neural Gain Control using Spike...\n",
      "2                          Competition Adds Complexity\n",
      "3    Efficient Principled Learning of Thin Junction...\n",
      "4       Regularized Boost for Semi-Supervised Learning\n",
      "Name: title, dtype: object\n",
      "0    Non-negative matrix factorization (NMF) has pr...\n",
      "1    Spike-triggered averaging techniques are effec...\n",
      "2    It is known that determinining whether a DEC-P...\n",
      "3    We present the first truly polynomial algorith...\n",
      "4    Semi-supervised inductive learning concerns ho...\n",
      "Name: abstract, dtype: object\n",
      "0    Algorithms for Non-negative Matrix\\nFactorizat...\n",
      "1    Characterizing neural gain control using\\nspik...\n",
      "2    Competition adds complexity\\n\\nJudy Goldsmith\\...\n",
      "3    Efficient Principled Learning of Thin Junction...\n",
      "4    Regularized Boost for Semi-Supervised Learning...\n",
      "Name: paper_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "text_columns = [\"title\", \"abstract\", \"paper_text\"]\n",
    "for column in text_columns:\n",
    "    pre_process_text(papers_df, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b7b97d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:42.691261Z",
     "start_time": "2023-08-02T17:47:42.682833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data. two different multi plicative algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules. one algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence. the monotonic  convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm. the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence. '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.abstract_processed[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d05f13e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:42.720010Z",
     "start_time": "2023-08-02T17:47:42.697413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>title_processed</th>\n",
       "      <th>abstract_processed</th>\n",
       "      <th>paper_text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1861</td>\n",
       "      <td>2000</td>\n",
       "      <td>Daniel D. Lee,H. Sebastian Seung</td>\n",
       "      <td>algorithms for non-negative matrix factorization</td>\n",
       "      <td>non-negative matrix factorization (nmf) has pr...</td>\n",
       "      <td>algorithms for non-negative matrix factorizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1975</td>\n",
       "      <td>2001</td>\n",
       "      <td>Odelia Schwartz,E.J. Chichilnisky,Eero P. Simo...</td>\n",
       "      <td>characterizing neural gain control using spike...</td>\n",
       "      <td>spike-triggered averaging techniques are effec...</td>\n",
       "      <td>characterizing neural gain control using spike...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3163</td>\n",
       "      <td>2007</td>\n",
       "      <td>Judy Goldsmith,Martin Mundhenk</td>\n",
       "      <td>competition adds complexity</td>\n",
       "      <td>it is known that determinining whether a dec-p...</td>\n",
       "      <td>competition adds complexity judy goldsmith dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3164</td>\n",
       "      <td>2007</td>\n",
       "      <td>Anton Chechetka,Carlos Guestrin</td>\n",
       "      <td>efficient principled learning of thin junction...</td>\n",
       "      <td>we present the first truly polynomial algorith...</td>\n",
       "      <td>efficient principled learning of thin junction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3167</td>\n",
       "      <td>2007</td>\n",
       "      <td>Ke Chen,Shihai Wang</td>\n",
       "      <td>regularized boost for semi-supervised learning</td>\n",
       "      <td>semi-supervised inductive learning concerns ho...</td>\n",
       "      <td>regularized boost for semi-supervised learning...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  year                                            authors  \\\n",
       "0      1861  2000                   Daniel D. Lee,H. Sebastian Seung   \n",
       "1      1975  2001  Odelia Schwartz,E.J. Chichilnisky,Eero P. Simo...   \n",
       "2      3163  2007                     Judy Goldsmith,Martin Mundhenk   \n",
       "3      3164  2007                    Anton Chechetka,Carlos Guestrin   \n",
       "4      3167  2007                                Ke Chen,Shihai Wang   \n",
       "\n",
       "                                     title_processed  \\\n",
       "0   algorithms for non-negative matrix factorization   \n",
       "1  characterizing neural gain control using spike...   \n",
       "2                        competition adds complexity   \n",
       "3  efficient principled learning of thin junction...   \n",
       "4     regularized boost for semi-supervised learning   \n",
       "\n",
       "                                  abstract_processed  \\\n",
       "0  non-negative matrix factorization (nmf) has pr...   \n",
       "1  spike-triggered averaging techniques are effec...   \n",
       "2  it is known that determinining whether a dec-p...   \n",
       "3  we present the first truly polynomial algorith...   \n",
       "4  semi-supervised inductive learning concerns ho...   \n",
       "\n",
       "                                paper_text_processed  \n",
       "0  algorithms for non-negative matrix factorizati...  \n",
       "1  characterizing neural gain control using spike...  \n",
       "2  competition adds complexity judy goldsmith dep...  \n",
       "3  efficient principled learning of thin junction...  \n",
       "4  regularized boost for semi-supervised learning...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df = papers_df.drop([\"title\", \"abstract\", \"paper_text\"], axis=1)\n",
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea936a1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:42.730115Z",
     "start_time": "2023-08-02T17:47:42.722920Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_df = papers_df.rename(columns={'title_processed':'title', 'abstract_processed': 'abstract', 'paper_text_processed': 'paper_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdac785d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:42.743688Z",
     "start_time": "2023-08-02T17:47:42.732696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1861</td>\n",
       "      <td>2000</td>\n",
       "      <td>Daniel D. Lee,H. Sebastian Seung</td>\n",
       "      <td>algorithms for non-negative matrix factorization</td>\n",
       "      <td>non-negative matrix factorization (nmf) has pr...</td>\n",
       "      <td>algorithms for non-negative matrix factorizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1975</td>\n",
       "      <td>2001</td>\n",
       "      <td>Odelia Schwartz,E.J. Chichilnisky,Eero P. Simo...</td>\n",
       "      <td>characterizing neural gain control using spike...</td>\n",
       "      <td>spike-triggered averaging techniques are effec...</td>\n",
       "      <td>characterizing neural gain control using spike...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3163</td>\n",
       "      <td>2007</td>\n",
       "      <td>Judy Goldsmith,Martin Mundhenk</td>\n",
       "      <td>competition adds complexity</td>\n",
       "      <td>it is known that determinining whether a dec-p...</td>\n",
       "      <td>competition adds complexity judy goldsmith dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3164</td>\n",
       "      <td>2007</td>\n",
       "      <td>Anton Chechetka,Carlos Guestrin</td>\n",
       "      <td>efficient principled learning of thin junction...</td>\n",
       "      <td>we present the first truly polynomial algorith...</td>\n",
       "      <td>efficient principled learning of thin junction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3167</td>\n",
       "      <td>2007</td>\n",
       "      <td>Ke Chen,Shihai Wang</td>\n",
       "      <td>regularized boost for semi-supervised learning</td>\n",
       "      <td>semi-supervised inductive learning concerns ho...</td>\n",
       "      <td>regularized boost for semi-supervised learning...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  year                                            authors  \\\n",
       "0      1861  2000                   Daniel D. Lee,H. Sebastian Seung   \n",
       "1      1975  2001  Odelia Schwartz,E.J. Chichilnisky,Eero P. Simo...   \n",
       "2      3163  2007                     Judy Goldsmith,Martin Mundhenk   \n",
       "3      3164  2007                    Anton Chechetka,Carlos Guestrin   \n",
       "4      3167  2007                                Ke Chen,Shihai Wang   \n",
       "\n",
       "                                               title  \\\n",
       "0   algorithms for non-negative matrix factorization   \n",
       "1  characterizing neural gain control using spike...   \n",
       "2                        competition adds complexity   \n",
       "3  efficient principled learning of thin junction...   \n",
       "4     regularized boost for semi-supervised learning   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  non-negative matrix factorization (nmf) has pr...   \n",
       "1  spike-triggered averaging techniques are effec...   \n",
       "2  it is known that determinining whether a dec-p...   \n",
       "3  we present the first truly polynomial algorith...   \n",
       "4  semi-supervised inductive learning concerns ho...   \n",
       "\n",
       "                                          paper_text  \n",
       "0  algorithms for non-negative matrix factorizati...  \n",
       "1  characterizing neural gain control using spike...  \n",
       "2  competition adds complexity judy goldsmith dep...  \n",
       "3  efficient principled learning of thin junction...  \n",
       "4  regularized boost for semi-supervised learning...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "861cf4a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:42.753769Z",
     "start_time": "2023-08-02T17:47:42.747780Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_df = papers_df.drop([\"paper_text\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9deb61f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:42.761534Z",
     "start_time": "2023-08-02T17:47:42.756733Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../secret/openai\") as f:\n",
    "    openai_secret = f.read().strip()\n",
    "    \n",
    "# PDF_FILE = \"../data/GenericEmailMarketting/merged_file.pdf\"\n",
    "\n",
    "# use import getpass instead\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_secret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "212a593a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:42.799087Z",
     "start_time": "2023-08-02T17:47:42.764620Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "llm.openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c21d7ef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:45.972991Z",
     "start_time": "2023-08-02T17:47:42.801567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "989397ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:47.392237Z",
     "start_time": "2023-08-02T17:47:45.976834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\nThe current Prime Minister of the United Kingdom is Boris Johnson.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Who is the current prime minister of Britain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7921616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:47.400933Z",
     "start_time": "2023-08-02T17:47:47.396312Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "def get_openai_embedding(text):\n",
    "   text_rep = text.replace(\"\\n\", \" \")\n",
    "   return embeddings_model.embed_documents([text_rep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a49a512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:47.413346Z",
     "start_time": "2023-08-02T17:47:47.404088Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence1 = \"i like summer\"\n",
    "sentence2 = \"Brocholi on pizza is probably not a good idea\"\n",
    "sentence3 = \"I love the warm weather outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f66b98be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.498000Z",
     "start_time": "2023-08-02T17:47:47.417953Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding1 = embeddings_model.embed_query(sentence1)\n",
    "embedding2 = embeddings_model.embed_query(sentence2)\n",
    "embedding3 = embeddings_model.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "094e1686",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.513912Z",
     "start_time": "2023-08-02T17:47:49.501831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7065412380498463\n",
      "0.7131223889702469\n",
      "0.875834698703543\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(embedding1, embedding2))\n",
    "print(np.dot(embedding2, embedding3))\n",
    "print(np.dot(embedding1, embedding3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606365d1",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dc5321d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.520946Z",
     "start_time": "2023-08-02T17:47:49.517609Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DataFrameLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efbdd543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.528166Z",
     "start_time": "2023-08-02T17:47:49.524801Z"
    }
   },
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(papers_df, page_content_column=\"abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d09697be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.900987Z",
     "start_time": "2023-08-02T17:47:49.530742Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use lazy load for larger table, which won't read the full table into memory \n",
    "page = loader.load()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b063e7e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.914915Z",
     "start_time": "2023-08-02T17:47:49.910071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data. two different multi plicative algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules. one algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence. the monotonic  convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm. the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence. '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92c593b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.923083Z",
     "start_time": "2023-08-02T17:47:49.917709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': 1861,\n",
       " 'year': 2000,\n",
       " 'authors': 'Daniel D. Lee,H. Sebastian Seung',\n",
       " 'title': 'algorithms for non-negative matrix factorization'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "620c0386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.930511Z",
     "start_time": "2023-08-02T17:47:49.925767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.Document"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993d2d2",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c56d3",
   "metadata": {},
   "source": [
    "### Why do we need to split the data\n",
    "1) Chatgpt and LLM have limits\n",
    "\n",
    "\n",
    "\n",
    "2) To allow for efficient search for vector spaces\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0aa73477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.936259Z",
     "start_time": "2023-08-02T17:47:49.933243Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6dfe267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.941611Z",
     "start_time": "2023-08-02T17:47:49.938831Z"
    }
   },
   "outputs": [],
   "source": [
    "r_spltter = RecursiveCharacterTextSplitter( # Set a really small chunk size, just to show.\n",
    "    chunk_size = 26,\n",
    "    chunk_overlap  = 4,\n",
    "    length_function = len,\n",
    "    #seperators=['\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 26,\n",
    "    chunk_overlap  = 4,\n",
    "    length_function = len,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90acef87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.946149Z",
     "start_time": "2023-08-02T17:47:49.943708Z"
    }
   },
   "outputs": [],
   "source": [
    "text_a2z = 'abcdefghijklmnopqrstuvwxyz'\n",
    "text_a2z_plus = 'abcdefghijklmnopqrstuvwxyz 12345678'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74f2a024",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.952040Z",
     "start_time": "2023-08-02T17:47:49.947849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_spltter.split_text(text_a2z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25a2e5da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.957920Z",
     "start_time": "2023-08-02T17:47:49.954075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', '12345678']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_spltter.split_text(text_a2z_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "491d7b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.963755Z",
     "start_time": "2023-08-02T17:47:49.959993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(text_a2z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e07b0f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.969571Z",
     "start_time": "2023-08-02T17:47:49.965807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz 12345678']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(text_a2z_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af1f09e",
   "metadata": {},
   "source": [
    "The issue is Character text splitter splits only on new lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89eebdef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.974960Z",
     "start_time": "2023-08-02T17:47:49.972117Z"
    }
   },
   "outputs": [],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 26,\n",
    "    chunk_overlap  = 4,\n",
    "    length_function = len,\n",
    "    separator=\" \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "012f4002",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.984096Z",
     "start_time": "2023-08-02T17:47:49.978793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', '12345678']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(text_a2z_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d14ea5dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.990244Z",
     "start_time": "2023-08-02T17:47:49.986280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data. two different multi plicative algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules. one algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence. the monotonic  convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm. the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence. '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract = page.page_content\n",
    "\n",
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7ba264c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:49.997636Z",
     "start_time": "2023-08-02T17:47:49.992771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non-negative matrix',\n",
       " 'factorization (nmf) has',\n",
       " 'has previously been shown',\n",
       " 'to  be a useful',\n",
       " 'decomposition for',\n",
       " 'for multivariate data.',\n",
       " 'two different multi',\n",
       " 'plicative algorithms for',\n",
       " 'for nmf are analyzed.',\n",
       " 'they differ only slightly',\n",
       " 'in  the multiplicative',\n",
       " 'factor used in the update',\n",
       " 'rules. one algorithm can',\n",
       " 'can be  shown to minimize',\n",
       " 'the conventional least',\n",
       " 'squares error while the',\n",
       " 'the other  minimizes the',\n",
       " 'the generalized',\n",
       " 'kullback-leibler',\n",
       " 'divergence. the monotonic',\n",
       " 'convergence of both',\n",
       " 'algorithms can be proven',\n",
       " 'using an auxiliary func',\n",
       " 'tion analogous to that',\n",
       " 'used for proving',\n",
       " 'convergence of the',\n",
       " 'the expectation',\n",
       " 'maximization algorithm.',\n",
       " 'the algorithms can also',\n",
       " 'be interpreted as diag',\n",
       " 'onally rescaled gradient',\n",
       " 'descent, where the',\n",
       " 'the rescaling factor is',\n",
       " 'is optimally  chosen to',\n",
       " 'to ensure convergence.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_spltter.split_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "844da99d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.003980Z",
     "start_time": "2023-08-02T17:47:49.999858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non-negative matrix',\n",
       " 'factorization (nmf) has',\n",
       " 'has previously been shown',\n",
       " 'to be a useful',\n",
       " 'decomposition for',\n",
       " 'for multivariate data. two',\n",
       " 'two different multi',\n",
       " 'plicative algorithms for',\n",
       " 'for nmf are analyzed. they',\n",
       " 'they differ only slightly',\n",
       " 'in the multiplicative',\n",
       " 'factor used in the update',\n",
       " 'rules. one algorithm can',\n",
       " 'can be shown to minimize',\n",
       " 'the conventional least',\n",
       " 'squares error while the',\n",
       " 'the other minimizes the',\n",
       " 'the generalized',\n",
       " 'kullback-leibler',\n",
       " 'divergence. the monotonic',\n",
       " 'convergence of both',\n",
       " 'both algorithms can be',\n",
       " 'be proven using an',\n",
       " 'an auxiliary func tion',\n",
       " 'tion analogous to that',\n",
       " 'that used for proving',\n",
       " 'convergence of the',\n",
       " 'the expectation',\n",
       " 'maximization algorithm.',\n",
       " 'the algorithms can also be',\n",
       " 'be interpreted as diag',\n",
       " 'diag onally rescaled',\n",
       " 'gradient descent, where',\n",
       " 'the rescaling factor is',\n",
       " 'is optimally chosen to',\n",
       " 'to ensure convergence.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6374d4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.023151Z",
     "start_time": "2023-08-02T17:47:50.006371Z"
    }
   },
   "outputs": [],
   "source": [
    "r_spltter = RecursiveCharacterTextSplitter( # Set a really small chunk size, just to show.\n",
    "   \n",
    "    length_function = len,\n",
    "    separators=['\\n\\n', \"\\n\", \" \", \"\"],\n",
    "     chunk_size = 150,\n",
    "    chunk_overlap  = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5770388e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.028849Z",
     "start_time": "2023-08-02T17:47:50.025337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data. two different multi plicative',\n",
       " 'algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules. one algorithm can be  shown to',\n",
       " 'minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence. the monotonic  convergence of',\n",
       " 'both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm.',\n",
       " 'the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure',\n",
       " 'convergence.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_spltter.split_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7177ee16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.033542Z",
     "start_time": "2023-08-02T17:47:50.030706Z"
    }
   },
   "outputs": [],
   "source": [
    "r_spltter = RecursiveCharacterTextSplitter( # Set a really small chunk size, just to show.\n",
    "    length_function = len,\n",
    "    separators=['\\n\\n', \"\\n\", \" \",\"\\.\", \"\"],\n",
    "     chunk_size = 1000,\n",
    "    chunk_overlap  = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b438550",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.039198Z",
     "start_time": "2023-08-02T17:47:50.035728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data. two different multi plicative algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules. one algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence. the monotonic  convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm. the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_spltter.split_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0b58bd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.043687Z",
     "start_time": "2023-08-02T17:47:50.041007Z"
    }
   },
   "outputs": [],
   "source": [
    "# what if we want to split by sentences\n",
    "# regex with look behind\n",
    "sentence_spltter = RecursiveCharacterTextSplitter( # Set a really small chunk size, just to show.\n",
    "    length_function = len,\n",
    "    separators=[\"(?<=\\.)\"],\n",
    "     chunk_size = 150,\n",
    "    chunk_overlap  = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b77495da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.049768Z",
     "start_time": "2023-08-02T17:47:50.045545Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data.',\n",
       " 'two different multi plicative algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules.',\n",
       " 'one algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence.',\n",
       " ' the monotonic  convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm.',\n",
       " ' the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_spltter.split_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34f6472a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.056892Z",
     "start_time": "2023-08-02T17:47:50.053636Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b8a069a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.419998Z",
     "start_time": "2023-08-02T17:47:50.060799Z"
    }
   },
   "outputs": [],
   "source": [
    "token_text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f7c5c83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.428501Z",
     "start_time": "2023-08-02T17:47:50.422585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc',\n",
       " 'def',\n",
       " 'gh',\n",
       " 'ij',\n",
       " 'kl',\n",
       " 'mn',\n",
       " 'op',\n",
       " 'q',\n",
       " 'r',\n",
       " 'st',\n",
       " 'uv',\n",
       " 'w',\n",
       " 'xy',\n",
       " 'z']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text_splitter.split_text(text_a2z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef5a7247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.438050Z",
     "start_time": "2023-08-02T17:47:50.430575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non',\n",
       " '-',\n",
       " 'negative',\n",
       " ' matrix',\n",
       " ' factor',\n",
       " 'ization',\n",
       " ' (',\n",
       " 'nm',\n",
       " 'f',\n",
       " ')',\n",
       " ' has',\n",
       " ' previously',\n",
       " ' been',\n",
       " ' shown',\n",
       " ' to',\n",
       " ' ',\n",
       " ' be',\n",
       " ' a',\n",
       " ' useful',\n",
       " ' decom',\n",
       " 'position',\n",
       " ' for',\n",
       " ' mult',\n",
       " 'ivariate',\n",
       " ' data',\n",
       " '.',\n",
       " ' two',\n",
       " ' different',\n",
       " ' multi',\n",
       " ' pl',\n",
       " 'icative',\n",
       " ' algorithms',\n",
       " ' for',\n",
       " ' nm',\n",
       " 'f',\n",
       " ' are',\n",
       " ' analyzed',\n",
       " '.',\n",
       " ' they',\n",
       " ' differ',\n",
       " ' only',\n",
       " ' slightly',\n",
       " ' in',\n",
       " ' ',\n",
       " ' the',\n",
       " ' multipl',\n",
       " 'icative',\n",
       " ' factor',\n",
       " ' used',\n",
       " ' in',\n",
       " ' the',\n",
       " ' update',\n",
       " ' rules',\n",
       " '.',\n",
       " ' one',\n",
       " ' algorithm',\n",
       " ' can',\n",
       " ' be',\n",
       " ' ',\n",
       " ' shown',\n",
       " ' to',\n",
       " ' minimize',\n",
       " ' the',\n",
       " ' conventional',\n",
       " ' least',\n",
       " ' squares',\n",
       " ' error',\n",
       " ' while',\n",
       " ' the',\n",
       " ' other',\n",
       " ' ',\n",
       " ' minim',\n",
       " 'izes',\n",
       " ' the',\n",
       " ' generalized',\n",
       " ' k',\n",
       " 'ull',\n",
       " 'back',\n",
       " '-',\n",
       " 'le',\n",
       " 'ib',\n",
       " 'ler',\n",
       " ' divergence',\n",
       " '.',\n",
       " ' the',\n",
       " ' mon',\n",
       " 'ot',\n",
       " 'onic',\n",
       " ' ',\n",
       " ' convergence',\n",
       " ' of',\n",
       " ' both',\n",
       " ' algorithms',\n",
       " ' can',\n",
       " ' be',\n",
       " ' proven',\n",
       " ' using',\n",
       " ' an',\n",
       " ' auxiliary',\n",
       " ' func',\n",
       " ' tion',\n",
       " ' analogous',\n",
       " ' to',\n",
       " ' that',\n",
       " ' used',\n",
       " ' for',\n",
       " ' proving',\n",
       " ' convergence',\n",
       " ' of',\n",
       " ' the',\n",
       " ' expectation',\n",
       " ' maxim',\n",
       " 'ization',\n",
       " ' algorithm',\n",
       " '.',\n",
       " ' the',\n",
       " ' algorithms',\n",
       " ' can',\n",
       " ' also',\n",
       " ' be',\n",
       " ' interpreted',\n",
       " ' as',\n",
       " ' di',\n",
       " 'ag',\n",
       " ' on',\n",
       " 'ally',\n",
       " ' resc',\n",
       " 'aled',\n",
       " ' gradient',\n",
       " ' descent',\n",
       " ',',\n",
       " ' where',\n",
       " ' the',\n",
       " ' resc',\n",
       " 'aling',\n",
       " ' factor',\n",
       " ' is',\n",
       " ' optim',\n",
       " 'ally',\n",
       " ' ',\n",
       " ' chosen',\n",
       " ' to',\n",
       " ' ensure',\n",
       " ' convergence',\n",
       " '.',\n",
       " ' ']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text_splitter.split_text(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abbbe69a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.759545Z",
     "start_time": "2023-08-02T17:47:50.440235Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b279b09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.766879Z",
     "start_time": "2023-08-02T17:47:50.762313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3921"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb678030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:50.772706Z",
     "start_time": "2023-08-02T17:47:50.768909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data. two different multi plicative algorithms for nmf are analyzed. they differ only slightly in  the multiplicative factor used in the update rules. one algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence. the monotonic  convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm. the algorithms can also be interpreted as diag onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence. ', metadata={'paper_id': 1861, 'year': 2000, 'authors': 'Daniel D. Lee,H. Sebastian Seung', 'title': 'algorithms for non-negative matrix factorization'})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f53ea8c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:51.232420Z",
     "start_time": "2023-08-02T17:47:50.774375Z"
    }
   },
   "outputs": [],
   "source": [
    "splits = sentence_spltter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6ee0239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:51.239018Z",
     "start_time": "2023-08-02T17:47:51.234698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data.', metadata={'paper_id': 1861, 'year': 2000, 'authors': 'Daniel D. Lee,H. Sebastian Seung', 'title': 'algorithms for non-negative matrix factorization'})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1d15f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:52.191458Z",
     "start_time": "2023-08-02T17:47:51.241352Z"
    }
   },
   "outputs": [],
   "source": [
    "random_splits = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b7e1dc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:52.198089Z",
     "start_time": "2023-08-02T17:47:52.195020Z"
    }
   },
   "outputs": [],
   "source": [
    "# For simplicity we will not use any splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345bde27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b511cfa7",
   "metadata": {},
   "source": [
    "## Indexing data in Vectorstores\n",
    "\n",
    "### ChromaDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "736d970c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:43:05.221449Z",
     "start_time": "2023-08-02T18:43:04.533069Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9565318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:52.202962Z",
     "start_time": "2023-08-02T17:47:52.199754Z"
    }
   },
   "outputs": [],
   "source": [
    "persist_directory_random_split_gpt = './data/chroma/random_split/gpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65b81159",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:47:52.231187Z",
     "start_time": "2023-08-02T17:47:52.205008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"rm -rf {persist_directory_random_split_gpt}\")  # remove old database files if any # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "069b47e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:59:01.906686Z",
     "start_time": "2023-08-02T17:47:52.234201Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: HTTP code 502 from API (<html>\r\n",
      "<head><title>502 Bad Gateway</title></head>\r\n",
      "<body>\r\n",
      "<center><h1>502 Bad Gateway</h1></center>\r\n",
      "<hr><center>cloudflare</center>\r\n",
      "</body>\r\n",
      "</html>\r\n",
      ").\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 2.58 s, total: 14.2 s\n",
      "Wall time: 11min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectordb_gpt = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=persist_directory_random_split_gpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "54ac24f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:59:01.922004Z",
     "start_time": "2023-08-02T17:59:01.912776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3921\n"
     ]
    }
   ],
   "source": [
    "print(vectordb_gpt._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "abd9068c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:59:02.986557Z",
     "start_time": "2023-08-02T17:59:01.924218Z"
    }
   },
   "outputs": [],
   "source": [
    "vectordb_gpt.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3fea2",
   "metadata": {},
   "source": [
    "### Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "21580e20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:59:03.174878Z",
     "start_time": "2023-08-02T17:59:02.989823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"name\" : \"a1b07c8dd9e0\",\r\n",
      "  \"cluster_name\" : \"docker-cluster\",\r\n",
      "  \"cluster_uuid\" : \"TCUdzVJbQ4CZ1WorXbYDog\",\r\n",
      "  \"version\" : {\r\n",
      "    \"number\" : \"8.8.2\",\r\n",
      "    \"build_flavor\" : \"default\",\r\n",
      "    \"build_type\" : \"docker\",\r\n",
      "    \"build_hash\" : \"98e1271edf932a480e4262a471281f1ee295ce6b\",\r\n",
      "    \"build_date\" : \"2023-06-26T05:16:16.196344851Z\",\r\n",
      "    \"build_snapshot\" : false,\r\n",
      "    \"lucene_version\" : \"9.6.0\",\r\n",
      "    \"minimum_wire_compatibility_version\" : \"7.17.0\",\r\n",
      "    \"minimum_index_compatibility_version\" : \"7.0.0\"\r\n",
      "  },\r\n",
      "  \"tagline\" : \"You Know, for Search\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!curl -X GET \"http://localhost:9200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d22025b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:59:03.347311Z",
     "start_time": "2023-08-02T17:59:03.180415Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import ElasticVectorSearch\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticKnnSearch\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "elastic = Elasticsearch(hosts=[\"http://localhost:9200\"])\n",
    "index_name = \"test_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7a96406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T17:59:06.178956Z",
     "start_time": "2023-08-02T17:59:03.349715Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/q4kkg14x6c9bg29754hkb_yr0000gn/T/ipykernel_68400/2402150112.py:1: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  elastic.delete_by_query(index=[index_name], body={\"query\": {\"match_all\": {}}})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'took': 2010, 'timed_out': False, 'total': 3952, 'deleted': 3952, 'batches': 4, 'version_conflicts': 0, 'noops': 0, 'retries': {'bulk': 0, 'search': 0}, 'throttled_millis': 0, 'requests_per_second': -1.0, 'throttled_until_millis': 0, 'failures': []})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic.delete_by_query(index=[index_name], body={\"query\": {\"match_all\": {}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "60a0f4ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:42.242739Z",
     "start_time": "2023-08-02T17:59:06.183103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.49 s, sys: 2.92 s, total: 10.4 s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "elasticdb_gpt = ElasticVectorSearch.from_documents(\n",
    "    docs,\n",
    "    embedding,\n",
    "    index_name=index_name,\n",
    "    elasticsearch_url=\"http://localhost:9200\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6302dbc1",
   "metadata": {},
   "source": [
    "go here http://localhost:5601/app/r/s/70gZ3 to see vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7ebfb775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:42.255067Z",
     "start_time": "2023-08-02T18:01:42.246238Z"
    }
   },
   "outputs": [],
   "source": [
    "elastic_vector_search = ElasticVectorSearch(\n",
    "            elasticsearch_url=\"http://localhost:9200\",\n",
    "            index_name=index_name,\n",
    "            embedding=embedding\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6608d527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:42.262145Z",
     "start_time": "2023-08-02T18:01:42.258458Z"
    }
   },
   "outputs": [],
   "source": [
    "elastic_vector_search_knn = ElasticKnnSearch(\n",
    "\n",
    "            es_connection=elasticdb_gpt.client,\n",
    "            index_name=index_name,\n",
    "            embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca66e3",
   "metadata": {},
   "source": [
    "##  Search\n",
    "\n",
    "When a query comes in we first convert it to a vector and then compare the vector to the elements in the database to get n most similar results.\n",
    "\n",
    "**Similarity Search**\n",
    "1. Similarity Search aims to find the most similar items to a given query in a dataset.\n",
    "2. It uses metrics like cosine similarity, Jaccard similarity, or Euclidean distance to quantify similarity.\n",
    "3. The search results are ranked based on their similarity scores, and the top-K items are returned.\n",
    "4. Use-cases: When the user's intent is clear and specific, similarity search is efficient. It's commonly used in standard search engines, recommendation systems, or any scenario where the goal is to find items most similar to the query.\n",
    "\n",
    "**Maximal Marginal Relevance (MMR) Search**\n",
    "1. MMR aims to provide a diverse set of results that are relevant to the query.\n",
    "2. Along with quantifying similarity to the query, MMR also considers similarity between items in the results set to ensure diversity.\n",
    "3. It aims to maximize the relevance of the returned items to the query, but also minimize the similarity between the returned items.\n",
    "4. Use-cases: When user intent is ambiguous or when there are multiple relevant responses, MMR can provide a more diverse set of results. It's useful in news article recommendation (to avoid recommending too many similar articles) or in conversational AI (to provide diverse responses).\n",
    "\n",
    "The choice between similarity search and MMR depends on the specific use case and user needs. If the aim is to provide a diverse set of results, MMR would be more suitable. If the goal is to find items most similar to the query, a similarity search would be more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93524545",
   "metadata": {},
   "source": [
    "### Keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "45f36b49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:42.435310Z",
     "start_time": "2023-08-02T18:01:42.264977Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#keyword search\n",
    "resp = elasticdb_gpt.client.search(q=\"What is Natural language processing\", query={\"match_all\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d9c1f4e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:42.451219Z",
     "start_time": "2023-08-02T18:01:42.436923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 10000 Hits:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cross language text classi?cation is an import...</td>\n",
       "      <td>20.207375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cross language text classi?cation is an import...</td>\n",
       "      <td>20.207375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classically, tasks in natural language process...</td>\n",
       "      <td>18.876570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td>18.226519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>our framework is inspired by state-of-the-art ...</td>\n",
       "      <td>17.344196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      score\n",
       "0  cross language text classi?cation is an import...  20.207375\n",
       "1  cross language text classi?cation is an import...  20.207375\n",
       "2  Classically, tasks in natural language process...  18.876570\n",
       "3  It obtains new state-of-the-art results on ele...  18.226519\n",
       "4  our framework is inspired by state-of-the-art ...  17.344196"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Got %d Hits:\" % resp['hits']['total']['value'])\n",
    "table = []\n",
    "for hit in resp['hits']['hits']:\n",
    "    table.append([hit['_source']['text'], hit['_score']])\n",
    "table_df = pd.DataFrame(table, columns=[\"text\", \"score\"])\n",
    "\n",
    "table_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0c2b60e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:47:23.642420Z",
     "start_time": "2023-08-02T18:47:23.636946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cross language text classi?cation is an important learning task in natural language processing.',\n",
       "  20.207375],\n",
       " ['cross language text classi?cation is an important learning task in natural language processing.',\n",
       "  20.207375],\n",
       " ['Classically, tasks in natural language processing have been performed through rule-based and statistical methodologies.',\n",
       "  18.87657],\n",
       " ['It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.',\n",
       "  18.226519],\n",
       " ['our framework is inspired by state-of-the-art smoothing techniques used in natural language processing (nlp).',\n",
       "  17.344196],\n",
       " ['our framework is inspired by state-of-the-art smoothing techniques used in natural language processing (nlp).',\n",
       "  17.344196],\n",
       " [' This survey represents an effort at providing a succinct yet complete understanding of the recent advances in natural language processing using deep learning in with a special focus on detailing transfer learning and its potential advantages.',\n",
       "  17.052074],\n",
       " ['learning latent representations from long text sequences is an important first step in many natural language processing applications.',\n",
       "  17.012213],\n",
       " ['learning latent representations from long text sequences is an important first step in many natural language processing applications.',\n",
       "  17.012213],\n",
       " [' Eventually, with the advent of advanced recurrent neural network architectures such as the LSTM, we were able to achieve state-of-the-art performance in several natural language processing tasks such as text classification and machine translation.',\n",
       "  16.973953]]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d78c45",
   "metadata": {},
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ffc2a094",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:43.108104Z",
     "start_time": "2023-08-02T18:01:42.479555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"a long-term goal of machine learning research is to build an intelligent dialog agent. most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). this kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. in this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. we study this setup in two domains: the babi dataset of (weston et al., 2015) and large-scale question answering from (dodge et al., 2015). we evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. in particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.\", metadata={'paper_id': 6264, 'year': 2016, 'authors': 'Jason E. Weston', 'title': 'dialog-based language learning'}),\n",
       " Document(page_content='teaching machines to read natural language documents remains an elusive challenge. machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. in this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. this allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.', metadata={'paper_id': 5945, 'year': 2015, 'authors': 'Karl Moritz Hermann,Tomas Kocisky,Edward Grefenstette,Lasse Espeholt,Will Kay,Mustafa Suleyman,Phil Blunsom', 'title': 'teaching machines to read and comprehend'}),\n",
       " Document(page_content=\"users want natural language processing (nlp) systems to be both fast and accurate, but quality often comes at the cost of speed. the field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets). we aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \\\\cite{kay-1986}. unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. an attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features. moreover, it is not specifically tuned for the known reward function. we propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines.\", metadata={'paper_id': 4556, 'year': 2012, 'authors': 'Jiarong Jiang,Adam Teichert,Jason Eisner,Hal Daume', 'title': 'learned prioritization for trading off accuracy and speed'}),\n",
       " Document(page_content='neural probabilistic language models (nplms) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. the main drawback of nplms is their extremely long training and testing times. morin and bengio have proposed a hierarchical language model built around a binary tree of words that was two orders of magnitude faster than the non-hierarchical language model it was based on. however, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. we introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. we then show that the resulting models can outperform non-hierarchical models and achieve state-of-the-art performance.', metadata={'paper_id': 3583, 'year': 2008, 'authors': 'Andriy Mnih,Geoffrey E. Hinton', 'title': 'a scalable hierarchical distributed language model'})]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.similarity_search(\"What is Natural language processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4d83dbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:44.155103Z",
     "start_time": "2023-08-02T18:01:43.111451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content=\"a long-term goal of machine learning research is to build an intelligent dialog agent. most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). this kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. in this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. we study this setup in two domains: the babi dataset of (weston et al., 2015) and large-scale question answering from (dodge et al., 2015). we evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. in particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.\", metadata={'paper_id': 6264, 'year': 2016, 'authors': 'Jason E. Weston', 'title': 'dialog-based language learning'}),\n",
       "  0.32908713817596436),\n",
       " (Document(page_content='teaching machines to read natural language documents remains an elusive challenge. machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. in this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. this allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.', metadata={'paper_id': 5945, 'year': 2015, 'authors': 'Karl Moritz Hermann,Tomas Kocisky,Edward Grefenstette,Lasse Espeholt,Will Kay,Mustafa Suleyman,Phil Blunsom', 'title': 'teaching machines to read and comprehend'}),\n",
       "  0.3368421494960785),\n",
       " (Document(page_content=\"users want natural language processing (nlp) systems to be both fast and accurate, but quality often comes at the cost of speed. the field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets). we aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \\\\cite{kay-1986}. unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. an attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features. moreover, it is not specifically tuned for the known reward function. we propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines.\", metadata={'paper_id': 4556, 'year': 2012, 'authors': 'Jiarong Jiang,Adam Teichert,Jason Eisner,Hal Daume', 'title': 'learned prioritization for trading off accuracy and speed'}),\n",
       "  0.3394159972667694),\n",
       " (Document(page_content='neural probabilistic language models (nplms) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. the main drawback of nplms is their extremely long training and testing times. morin and bengio have proposed a hierarchical language model built around a binary tree of words that was two orders of magnitude faster than the non-hierarchical language model it was based on. however, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. we introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. we then show that the resulting models can outperform non-hierarchical models and achieve state-of-the-art performance.', metadata={'paper_id': 3583, 'year': 2008, 'authors': 'Andriy Mnih,Geoffrey E. Hinton', 'title': 'a scalable hierarchical distributed language model'}),\n",
       "  0.3485580384731293)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.similarity_search_with_score(\"What is Natural language processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "470baeb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:44.621003Z",
     "start_time": "2023-08-02T18:01:44.158258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='when used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. when there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients. empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. we propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization. we demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. we also develop a theory that motivates the algorithm.', metadata={'paper_id': 3686, 'year': 2009, 'authors': 'Yi-hao Kao,Benjamin V. Roy,Xiang Yan', 'title': 'directed regression'}),\n",
       " Document(page_content='linear regression studies the problem of estimating a model parameter $\\\\beta^* \\\\in \\\\r^p$, from $n$ observations $\\\\{(y_i,x_i)\\\\}_{i=1}^n$ from linear model $y_i = \\\\langle \\\\x_i,\\\\beta^* \\\\rangle + \\\\epsilon_i$. we consider a significant generalization in which the relationship between $\\\\langle x_i,\\\\beta^* \\\\rangle$ and $y_i$ is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as well as unknown. this model is known as the single-index model in statistics, and, among other things, it represents a significant generalization of one-bit compressed sensing. we propose a novel spectral-based estimation procedure and show that we can recover $\\\\beta^*$ in settings (i.e., classes of link function $f$) where previous algorithms fail. in general, our algorithm requires only very mild restrictions on the (unknown) functional relationship between $y_i$ and $\\\\langle x_i,\\\\beta^* \\\\rangle$. we also consider the high dimensional setting where $\\\\beta^*$ is sparse, and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where $p \\\\gg n$. for a broad class of link functions between $\\\\langle x_i,\\\\beta^* \\\\rangle$ and $y_i$, we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes.', metadata={'paper_id': 6013, 'year': 2015, 'authors': 'Xinyang Yi,Zhaoran Wang,Constantine Caramanis,Han Liu', 'title': 'optimal linear estimation under unknown nonlinear transform'}),\n",
       " Document(page_content='this article considers algorithmic and statistical aspects of linear regression when the correspondence between the covariates and the responses is unknown. first, a fully polynomial-time approximation scheme is given for the natural least squares optimization problem in any constant dimension. next, in an average-case and noise-free setting where the responses exactly correspond to a linear function of i.i.d. draws from a standard multivariate normal distribution, an efficient algorithm based on lattice basis reduction is shown to exactly recover the unknown linear function in arbitrary dimension. finally, lower bounds on the signal-to-noise ratio are established for approximate recovery of the unknown linear function by any estimator.', metadata={'paper_id': 6751, 'year': 2017, 'authors': 'Daniel J. Hsu,Kevin Shi,Xiaorui Sun', 'title': 'linear regression without correspondence'}),\n",
       " Document(page_content='in a regression task, a predictor is given a set of instances, along with a real value for each point. subsequently, she has to identify the value of a new instance as accurately as possible. in this work, we initiate the study of strategic predictions in machine learning. we consider a regression task tackled by two players, where the payoff of each player is the proportion of the points she predicts more accurately than the other player. we first revise the probably approximately correct learning framework to deal with the case of a duel between two predictors. we then devise an algorithm which finds a linear regression predictor that is a best response to any (not necessarily linear) regression algorithm. we show that it has linearithmic sample complexity, and polynomial time complexity when the dimension of the instances domain is fixed. we also test our approach in a high-dimensional setting, and show it significantly defeats classical regression algorithms in the prediction duel. together, our work introduces a novel machine learning task that lends itself well to current competitive online settings, provides its theoretical foundations, and illustrates its applicability.', metadata={'paper_id': 6748, 'year': 2017, 'authors': 'Omer Ben Porat,Moshe Tennenholtz', 'title': 'best response regression'})]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.similarity_search(\"What is linear regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ae07bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:45.501059Z",
     "start_time": "2023-08-02T18:01:44.624217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content=\"a long-term goal of machine learning research is to build an intelligent dialog agent. most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). this kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. in this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. we study this setup in two domains: the babi dataset of (weston et al., 2015) and large-scale question answering from (dodge et al., 2015). we evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. in particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.\", metadata={'paper_id': 6264, 'year': 2016, 'authors': 'Jason E. Weston', 'title': 'dialog-based language learning'}),\n",
       "  1.8354564),\n",
       " (Document(page_content='teaching machines to read natural language documents remains an elusive challenge. machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. in this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. this allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.', metadata={'paper_id': 5945, 'year': 2015, 'authors': 'Karl Moritz Hermann,Tomas Kocisky,Edward Grefenstette,Lasse Espeholt,Will Kay,Mustafa Suleyman,Phil Blunsom', 'title': 'teaching machines to read and comprehend'}),\n",
       "  1.8316371),\n",
       " (Document(page_content=\"users want natural language processing (nlp) systems to be both fast and accurate, but quality often comes at the cost of speed. the field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets). we aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \\\\cite{kay-1986}. unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. an attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features. moreover, it is not specifically tuned for the known reward function. we propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines.\", metadata={'paper_id': 4556, 'year': 2012, 'authors': 'Jiarong Jiang,Adam Teichert,Jason Eisner,Hal Daume', 'title': 'learned prioritization for trading off accuracy and speed'}),\n",
       "  1.830435),\n",
       " (Document(page_content='neural probabilistic language models (nplms) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. the main drawback of nplms is their extremely long training and testing times. morin and bengio have proposed a hierarchical language model built around a binary tree of words that was two orders of magnitude faster than the non-hierarchical language model it was based on. however, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. we introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. we then show that the resulting models can outperform non-hierarchical models and achieve state-of-the-art performance.', metadata={'paper_id': 3583, 'year': 2008, 'authors': 'Andriy Mnih,Geoffrey E. Hinton', 'title': 'a scalable hierarchical distributed language model'}),\n",
       "  1.8256913)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticdb_gpt.similarity_search_with_score(\"What is Natural language processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "71f8d296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:46.218663Z",
     "start_time": "2023-08-02T18:01:45.503250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content=\"a long-term goal of machine learning research is to build an intelligent dialog agent. most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). this kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. in this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. we study this setup in two domains: the babi dataset of (weston et al., 2015) and large-scale question answering from (dodge et al., 2015). we evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. in particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.\", metadata={'paper_id': 6264, 'year': 2016, 'authors': 'Jason E. Weston', 'title': 'dialog-based language learning'}),\n",
       "  1.8354564),\n",
       " (Document(page_content='teaching machines to read natural language documents remains an elusive challenge. machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. in this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. this allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.', metadata={'paper_id': 5945, 'year': 2015, 'authors': 'Karl Moritz Hermann,Tomas Kocisky,Edward Grefenstette,Lasse Espeholt,Will Kay,Mustafa Suleyman,Phil Blunsom', 'title': 'teaching machines to read and comprehend'}),\n",
       "  1.8316371),\n",
       " (Document(page_content=\"users want natural language processing (nlp) systems to be both fast and accurate, but quality often comes at the cost of speed. the field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets). we aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \\\\cite{kay-1986}. unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. an attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features. moreover, it is not specifically tuned for the known reward function. we propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines.\", metadata={'paper_id': 4556, 'year': 2012, 'authors': 'Jiarong Jiang,Adam Teichert,Jason Eisner,Hal Daume', 'title': 'learned prioritization for trading off accuracy and speed'}),\n",
       "  1.830435),\n",
       " (Document(page_content='neural probabilistic language models (nplms) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. the main drawback of nplms is their extremely long training and testing times. morin and bengio have proposed a hierarchical language model built around a binary tree of words that was two orders of magnitude faster than the non-hierarchical language model it was based on. however, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. we introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. we then show that the resulting models can outperform non-hierarchical models and achieve state-of-the-art performance.', metadata={'paper_id': 3583, 'year': 2008, 'authors': 'Andriy Mnih,Geoffrey E. Hinton', 'title': 'a scalable hierarchical distributed language model'}),\n",
       "  1.8256913)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_vector_search_knn.similarity_search_with_score(\"What is Natural language processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3099ef",
   "metadata": {},
   "source": [
    "### Maximum marginal relevance\n",
    "\n",
    "Maximum marginal relevance strives to achieve both relevance to the query and diversity among the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c764326d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:46.640253Z",
     "start_time": "2023-08-02T18:01:46.221121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='when used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. when there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients. empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. we propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization. we demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. we also develop a theory that motivates the algorithm.', metadata={'paper_id': 3686, 'year': 2009, 'authors': 'Yi-hao Kao,Benjamin V. Roy,Xiang Yan', 'title': 'directed regression'}),\n",
       " Document(page_content='linear regression studies the problem of estimating a model parameter $\\\\beta^* \\\\in \\\\r^p$, from $n$ observations $\\\\{(y_i,x_i)\\\\}_{i=1}^n$ from linear model $y_i = \\\\langle \\\\x_i,\\\\beta^* \\\\rangle + \\\\epsilon_i$. we consider a significant generalization in which the relationship between $\\\\langle x_i,\\\\beta^* \\\\rangle$ and $y_i$ is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as well as unknown. this model is known as the single-index model in statistics, and, among other things, it represents a significant generalization of one-bit compressed sensing. we propose a novel spectral-based estimation procedure and show that we can recover $\\\\beta^*$ in settings (i.e., classes of link function $f$) where previous algorithms fail. in general, our algorithm requires only very mild restrictions on the (unknown) functional relationship between $y_i$ and $\\\\langle x_i,\\\\beta^* \\\\rangle$. we also consider the high dimensional setting where $\\\\beta^*$ is sparse, and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where $p \\\\gg n$. for a broad class of link functions between $\\\\langle x_i,\\\\beta^* \\\\rangle$ and $y_i$, we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes.', metadata={'paper_id': 6013, 'year': 2015, 'authors': 'Xinyang Yi,Zhaoran Wang,Constantine Caramanis,Han Liu', 'title': 'optimal linear estimation under unknown nonlinear transform'}),\n",
       " Document(page_content='this paper proposes an efficient algorithm (holrr) to handle regression tasks where the outputs have a tensor structure. we formulate the regression problem as the minimization of a least square criterion under a multilinear rank constraint, a difficult non convex problem. holrr computes efficiently an approximate solution of this problem, with solid theoretical guarantees. a kernel extension is also presented. experiments on synthetic and real data show that holrr computes accurate solutions while being computationally very competitive.', metadata={'paper_id': 6302, 'year': 2016, 'authors': 'Guillaume Rabusseau,Hachem Kadri', 'title': 'low-rank regression with tensor responses'}),\n",
       " Document(page_content='the development of statistical models for continuous-time longitudinal network data is of increasing interest in machine learning and social science. leveraging ideas from survival and event history analysis, we introduce a continuous-time regression modeling framework for network event data that can incorporate both time-dependent network statistics and time-varying regression coefficients. we also develop an efficient inference scheme that allows our approach to scale to large networks. on synthetic and real-world data, empirical results demonstrate that the proposed inference approach can accurately estimate the coefficients of the regression model, which is useful for interpreting the evolution of the network; furthermore, the learned model has systematically better predictive performance compared to standard baseline methods.', metadata={'paper_id': 4436, 'year': 2011, 'authors': 'Duy Q. Vu,David Hunter,Padhraic Smyth,Arthur U. Asuncion', 'title': 'continuous-time regression models for longitudinal networks'})]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.max_marginal_relevance_search(\"What is linear regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a35827dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:47.890657Z",
     "start_time": "2023-08-02T18:01:46.643156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"a long-term goal of machine learning research is to build an intelligent dialog agent. most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). this kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. in this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. we study this setup in two domains: the babi dataset of (weston et al., 2015) and large-scale question answering from (dodge et al., 2015). we evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. in particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.\", metadata={'paper_id': 6264, 'year': 2016, 'authors': 'Jason E. Weston', 'title': 'dialog-based language learning'}),\n",
       " Document(page_content='it is commonly assumed that language refers to high-level visual concepts while leaving low-level visual processing unaffected. this view dominates the current literature in computational models for language-vision tasks, where visual and linguistic inputs are mostly processed independently before being fused into a single representation. in this paper, we deviate from this classic pipeline and propose to modulate the \\\\emph{entire visual processing} by a linguistic input. specifically, we introduce conditional batch normalization (cbn) as an efficient mechanism to modulate convolutional feature maps by a linguistic embedding. we apply cbn to a pre-trained residual network (resnet), leading to the modulated resnet (\\\\mrn) architecture, and show that this significantly improves strong baselines on two visual question answering tasks. our ablation study confirms that modulating from the early stages of the visual processing is beneficial.', metadata={'paper_id': 7237, 'year': 2017, 'authors': 'Harm de Vries,Florian Strub,Jeremie Mary,Hugo Larochelle,Olivier Pietquin,Aaron C. Courville', 'title': 'modulating early visual processing by language'}),\n",
       " Document(page_content='neural probabilistic language models (nplms) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. the main drawback of nplms is their extremely long training and testing times. morin and bengio have proposed a hierarchical language model built around a binary tree of words that was two orders of magnitude faster than the non-hierarchical language model it was based on. however, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. we introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. we then show that the resulting models can outperform non-hierarchical models and achieve state-of-the-art performance.', metadata={'paper_id': 3583, 'year': 2008, 'authors': 'Andriy Mnih,Geoffrey E. Hinton', 'title': 'a scalable hierarchical distributed language model'}),\n",
       " Document(page_content='natural sounds are structured on many time-scales. a typical segment of speech, for example, contains features that span four orders of magnitude: sentences (~1s); phonemes (~0.1s); glottal pulses (~0.01s); and formants (<0.001s). the auditory system uses information from each of these time-scales to solve complicated tasks such as auditory scene analysis. one route toward understanding how auditory processing accomplishes this analysis is to build neuroscience-inspired algorithms which solve similar tasks and to compare the properties of these algorithms with properties of auditory processing. there is however a discord: current machine-audition algorithms largely concentrate on the shorter time-scale structures in sounds, and the longer structures are ignored. the reason for this is two-fold. firstly, it is a difficult technical problem to construct an algorithm that utilises both sorts of information. secondly, it is computationally demanding to simultaneously process data both at high resolution (to extract short temporal information) and for long duration (to extract long temporal information). the contribution of this work is to develop a new statistical model for natural sounds that captures structure across a wide range of time-scales, and to provide efficient learning and inference algorithms. we demonstrate the success of this approach on a missing data task.', metadata={'paper_id': 3366, 'year': 2007, 'authors': 'Richard Turner,Maneesh Sahani', 'title': 'modeling natural sounds with modulation cascade processes'})]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.max_marginal_relevance_search(\"What is natural language processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "46b9386f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:49:48.513806Z",
     "start_time": "2023-08-02T18:49:47.564373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='neural probabilistic language models (nplms) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. the main drawback of nplms is their extremely long training and testing times. morin and bengio have proposed a hierarchical language model built around a binary tree of words that was two orders of magnitude faster than the non-hierarchical language model it was based on. however, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. we introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. we then show that the resulting models can outperform non-hierarchical models and achieve state-of-the-art performance.', metadata={'paper_id': 3583, 'year': 2008, 'authors': 'Andriy Mnih,Geoffrey E. Hinton', 'title': 'a scalable hierarchical distributed language model'}),\n",
       " Document(page_content='offline handwriting recognition---the transcription of images of handwritten text---is an interesting task, in that it combines computer vision with sequence learning. in most systems the two elements are handled separately, with sophisticated preprocessing techniques used to extract the image features and sequential models such as hmms used to provide the transcriptions. by combining two recent innovations in neural networks---multidimensional recurrent neural networks and connectionist temporal classification---this paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input. unlike competing systems, it does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language. evidence of its generality and power is provided by data from a recent international arabic recognition competition, where it outperformed all entries (91.4% accuracy compared to 87.2% for the competition winner) despite the fact that neither author understands a word of arabic.', metadata={'paper_id': 3449, 'year': 2008, 'authors': 'Alex Graves,J?rgen Schmidhuber', 'title': 'offline handwriting recognition with multidimensional recurrent neural networks'}),\n",
       " Document(page_content='we investigate a topic at the interface of machine learning and cognitive science. human active learning, where learners can actively query the world for information, is contrasted with passive learning from random examples. furthermore, we compare human active learning performance with predictions from statistical learning theory. we conduct a series of human category learning experiments inspired by a machine learning task for which active and passive learning error bounds are well understood, and dramatically distinct. our results indicate that humans are capable of actively selecting informative queries, and in doing so learn better and faster than if they are given random training data, as predicted by learning theory. however, the improvement over passive learning is not as dramatic as that achieved by machine active learning algorithms. to the best of our knowledge, this is the first quantitative study comparing human category learning in active versus passive settings.', metadata={'paper_id': 3456, 'year': 2008, 'authors': 'Rui M. Castro,Charles Kalish,Robert Nowak,Ruichen Qian,Tim Rogers,Xiaojin Zhu', 'title': 'human active learning'}),\n",
       " Document(page_content='working memory is a central topic of cognitive neuroscience because it is critical for solving real world problems in which information from multiple temporally distant sources must be combined to generate appropriate behavior. however, an often neglected fact is that learning to use working memory effectively is itself a difficult problem. the gating\" framework is a collection of psychological models that show how dopamine can train the basal ganglia and prefrontal cortex to form useful working memory representations in certain types of problems. we bring together gating with ideas from machine learning about using finite memory systems in more general problems. thus we present a normative gating model that learns, by online temporal difference methods, to use working memory to maximize discounted future rewards in general partially observable settings. the model successfully solves a benchmark working memory problem, and exhibits limitations similar to those observed in human experiments. moreover, the model introduces a concise, normative definition of high level cognitive concepts such as working memory and cognitive control in terms of maximizing discounted future rewards.\"', metadata={'paper_id': 3508, 'year': 2008, 'authors': 'Michael T. Todd,Yael Niv,Jonathan D. Cohen', 'title': 'learning to use working memory in partially observable environments through dopaminergic reinforcement'})]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.max_marginal_relevance_search(\"What is natural language processing\", filter={\"year\":2008})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8e95f320",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:48.363069Z",
     "start_time": "2023-08-02T18:01:47.894015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.max_marginal_relevance_search(\"What is natural language processing\", filter={\"year\":1990})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc965eae",
   "metadata": {},
   "source": [
    "## Question and Answer\n",
    "\n",
    "\n",
    "When a query comes in we first convert it to a vector and then compare the vector to the elements in the database to get n most similar results. These results are then passed into prompt as a context for LLM to process them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "069a4966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:48.369048Z",
     "start_time": "2023-08-02T18:01:48.365606Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "llm.openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dd39a96d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:53:02.299037Z",
     "start_time": "2023-08-02T18:53:02.284192Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know. Use three sentences maximum. Keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7980c1f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:52.697606Z",
     "start_time": "2023-08-02T18:01:48.378189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Linear regression is a statistical technique used to model the relationship between a response variable and one or more predictor variables. It involves estimating regression coefficients via ordinary least squares and using them to make decisions. It can also be used to predict the value of a response variable given a set of predictor variables.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "question = \"What is linear regression?\"\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=elasticdb_gpt.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "29b181e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:01:52.712644Z",
     "start_time": "2023-08-02T18:01:52.701109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='when used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. when there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients. empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. we propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization. we demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. we also develop a theory that motivates the algorithm.', metadata={'paper_id': 3686, 'year': 2009, 'authors': 'Yi-hao Kao,Benjamin V. Roy,Xiang Yan', 'title': 'directed regression'}),\n",
       " Document(page_content='this article considers algorithmic and statistical aspects of linear regression when the correspondence between the covariates and the responses is unknown. first, a fully polynomial-time approximation scheme is given for the natural least squares optimization problem in any constant dimension. next, in an average-case and noise-free setting where the responses exactly correspond to a linear function of i.i.d. draws from a standard multivariate normal distribution, an efficient algorithm based on lattice basis reduction is shown to exactly recover the unknown linear function in arbitrary dimension. finally, lower bounds on the signal-to-noise ratio are established for approximate recovery of the unknown linear function by any estimator.', metadata={'paper_id': 6751, 'year': 2017, 'authors': 'Daniel J. Hsu,Kevin Shi,Xiaorui Sun', 'title': 'linear regression without correspondence'}),\n",
       " Document(page_content='linear regression studies the problem of estimating a model parameter $\\\\beta^* \\\\in \\\\r^p$, from $n$ observations $\\\\{(y_i,x_i)\\\\}_{i=1}^n$ from linear model $y_i = \\\\langle \\\\x_i,\\\\beta^* \\\\rangle + \\\\epsilon_i$. we consider a significant generalization in which the relationship between $\\\\langle x_i,\\\\beta^* \\\\rangle$ and $y_i$ is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as well as unknown. this model is known as the single-index model in statistics, and, among other things, it represents a significant generalization of one-bit compressed sensing. we propose a novel spectral-based estimation procedure and show that we can recover $\\\\beta^*$ in settings (i.e., classes of link function $f$) where previous algorithms fail. in general, our algorithm requires only very mild restrictions on the (unknown) functional relationship between $y_i$ and $\\\\langle x_i,\\\\beta^* \\\\rangle$. we also consider the high dimensional setting where $\\\\beta^*$ is sparse, and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where $p \\\\gg n$. for a broad class of link functions between $\\\\langle x_i,\\\\beta^* \\\\rangle$ and $y_i$, we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes.', metadata={'paper_id': 6013, 'year': 2015, 'authors': 'Xinyang Yi,Zhaoran Wang,Constantine Caramanis,Han Liu', 'title': 'optimal linear estimation under unknown nonlinear transform'}),\n",
       " Document(page_content='in a regression task, a predictor is given a set of instances, along with a real value for each point. subsequently, she has to identify the value of a new instance as accurately as possible. in this work, we initiate the study of strategic predictions in machine learning. we consider a regression task tackled by two players, where the payoff of each player is the proportion of the points she predicts more accurately than the other player. we first revise the probably approximately correct learning framework to deal with the case of a duel between two predictors. we then devise an algorithm which finds a linear regression predictor that is a best response to any (not necessarily linear) regression algorithm. we show that it has linearithmic sample complexity, and polynomial time complexity when the dimension of the instances domain is fixed. we also test our approach in a high-dimensional setting, and show it significantly defeats classical regression algorithms in the prediction duel. together, our work introduces a novel machine learning task that lends itself well to current competitive online settings, provides its theoretical foundations, and illustrates its applicability.', metadata={'paper_id': 6748, 'year': 2017, 'authors': 'Omer Ben Porat,Moshe Tennenholtz', 'title': 'best response regression'}),\n",
       " Document(page_content='online sparse linear regression is the task of applying linear regression analysis to examples arriving sequentially subject to a resource constraint that a limited number of features of examples can be observed. despite its importance in many practical applications, it has been recently shown that there is no polynomial-time sublinear-regret algorithm unless np$\\\\subseteq$bpp, and only an exponential-time sublinear-regret algorithm has been found. in this paper, we introduce mild assumptions to solve the problem. under these assumptions, we present polynomial-time sublinear-regret algorithms for the online sparse linear regression. in addition, thorough experiments with publicly available data demonstrate that our algorithms outperform other known algorithms.', metadata={'paper_id': 6998, 'year': 2017, 'authors': 'Shinji Ito,Daisuke Hatano,Hanna Sumita,Akihiro Yabe,Takuro Fukunaga,Naonori Kakimura,Ken-Ichi Kawarabayashi', 'title': 'efficient sublinear-regret algorithms for online sparse linear regression with limited observation'})]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "877de9f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:02:23.823243Z",
     "start_time": "2023-08-02T18:01:52.715425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is linear regression?',\n",
       " 'result': ' Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. It is also used for function estimation and model selection in high-dimensional data analysis.',\n",
       " 'source_documents': [Document(page_content='when used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. when there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients. empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. we propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization. we demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. we also develop a theory that motivates the algorithm.', metadata={'paper_id': 3686, 'year': 2009, 'authors': 'Yi-hao Kao,Benjamin V. Roy,Xiang Yan', 'title': 'directed regression'}),\n",
       "  Document(page_content='this article considers algorithmic and statistical aspects of linear regression when the correspondence between the covariates and the responses is unknown. first, a fully polynomial-time approximation scheme is given for the natural least squares optimization problem in any constant dimension. next, in an average-case and noise-free setting where the responses exactly correspond to a linear function of i.i.d. draws from a standard multivariate normal distribution, an efficient algorithm based on lattice basis reduction is shown to exactly recover the unknown linear function in arbitrary dimension. finally, lower bounds on the signal-to-noise ratio are established for approximate recovery of the unknown linear function by any estimator.', metadata={'paper_id': 6751, 'year': 2017, 'authors': 'Daniel J. Hsu,Kevin Shi,Xiaorui Sun', 'title': 'linear regression without correspondence'}),\n",
       "  Document(page_content='linear regression studies the problem of estimating a model parameter $\\\\beta^* \\\\in \\\\r^p$, from $n$ observations $\\\\{(y_i,x_i)\\\\}_{i=1}^n$ from linear model $y_i = \\\\langle \\\\x_i,\\\\beta^* \\\\rangle + \\\\epsilon_i$. we consider a significant generalization in which the relationship between $\\\\langle x_i,\\\\beta^* \\\\rangle$ and $y_i$ is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as well as unknown. this model is known as the single-index model in statistics, and, among other things, it represents a significant generalization of one-bit compressed sensing. we propose a novel spectral-based estimation procedure and show that we can recover $\\\\beta^*$ in settings (i.e., classes of link function $f$) where previous algorithms fail. in general, our algorithm requires only very mild restrictions on the (unknown) functional relationship between $y_i$ and $\\\\langle x_i,\\\\beta^* \\\\rangle$. we also consider the high dimensional setting where $\\\\beta^*$ is sparse, and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where $p \\\\gg n$. for a broad class of link functions between $\\\\langle x_i,\\\\beta^* \\\\rangle$ and $y_i$, we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes.', metadata={'paper_id': 6013, 'year': 2015, 'authors': 'Xinyang Yi,Zhaoran Wang,Constantine Caramanis,Han Liu', 'title': 'optimal linear estimation under unknown nonlinear transform'}),\n",
       "  Document(page_content='in a regression task, a predictor is given a set of instances, along with a real value for each point. subsequently, she has to identify the value of a new instance as accurately as possible. in this work, we initiate the study of strategic predictions in machine learning. we consider a regression task tackled by two players, where the payoff of each player is the proportion of the points she predicts more accurately than the other player. we first revise the probably approximately correct learning framework to deal with the case of a duel between two predictors. we then devise an algorithm which finds a linear regression predictor that is a best response to any (not necessarily linear) regression algorithm. we show that it has linearithmic sample complexity, and polynomial time complexity when the dimension of the instances domain is fixed. we also test our approach in a high-dimensional setting, and show it significantly defeats classical regression algorithms in the prediction duel. together, our work introduces a novel machine learning task that lends itself well to current competitive online settings, provides its theoretical foundations, and illustrates its applicability.', metadata={'paper_id': 6748, 'year': 2017, 'authors': 'Omer Ben Porat,Moshe Tennenholtz', 'title': 'best response regression'}),\n",
       "  Document(page_content='online sparse linear regression is the task of applying linear regression analysis to examples arriving sequentially subject to a resource constraint that a limited number of features of examples can be observed. despite its importance in many practical applications, it has been recently shown that there is no polynomial-time sublinear-regret algorithm unless np$\\\\subseteq$bpp, and only an exponential-time sublinear-regret algorithm has been found. in this paper, we introduce mild assumptions to solve the problem. under these assumptions, we present polynomial-time sublinear-regret algorithms for the online sparse linear regression. in addition, thorough experiments with publicly available data demonstrate that our algorithms outperform other known algorithms.', metadata={'paper_id': 6998, 'year': 2017, 'authors': 'Shinji Ito,Daisuke Hatano,Hanna Sumita,Akihiro Yabe,Takuro Fukunaga,Naonori Kakimura,Ken-Ichi Kawarabayashi', 'title': 'efficient sublinear-regret algorithms for online sparse linear regression with limited observation'}),\n",
       "  Document(page_content='log-linear models are widely used probability models for statistical pattern recognition. typically, log-linear models are trained according to a convex criterion. in recent years, the interest in log-linear models has greatly increased. the optimization of log-linear model parameters is costly and therefore an important topic, in particular for large-scale applications. different optimization algorithms have been evaluated empirically in many papers. in this work, we analyze the optimization problem analytically and show that the training of log-linear models can be highly ill-conditioned. we verify our findings on two handwriting tasks. by making use of our convergence analysis, we obtain good results on a large-scale continuous handwriting recognition task with a simple and generic approach.', metadata={'paper_id': 4421, 'year': 2011, 'authors': 'Simon Wiesler,Hermann Ney', 'title': 'a convergence analysis of log-linear training'}),\n",
       "  Document(page_content='we study regression and classification in a setting where the learning algorithm is allowed to access only a limited number of attributes per example, known as the limited attribute observation model. in this well-studied model, we provide the first lower bounds giving a limit on the precision attainable by any algorithm for several variants of regression, notably linear regression with the absolute loss and the squared loss, as well as for classification with the hinge loss. we complement these lower bounds with a general purpose algorithm that gives an upper bound on the achievable precision limit in the setting of learning with missing data.', metadata={'paper_id': 6171, 'year': 2016, 'authors': 'Brian Bullins,Elad Hazan,Tomer Koren', 'title': 'the limits of learning with missing data'}),\n",
       "  Document(page_content='we present a new empirical risk minimization framework for approximating functions from training samples for low-dimensional regression applications where a lattice (look-up table) is stored and interpolated at run-time for an efficient hardware implementation. rather than evaluating a fitted function at the lattice nodes without regard to the fact that samples will be interpolated, the proposed lattice regression approach estimates the lattice to minimize the interpolation error on the given training samples. experiments show that lattice regression can reduce mean test error compared to gaussian process regression for digital color management of printers, an application for which linearly interpolating a look-up table (lut) is standard. simulations confirm that lattice regression performs consistently better than the naive approach to learning the lattice, particularly when the density of training samples is low.', metadata={'paper_id': 3694, 'year': 2009, 'authors': 'Eric Garcia,Maya Gupta', 'title': 'lattice regression'}),\n",
       "  Document(page_content='we consider linear models for stochastic dynamics. any such model can be associated a network (namely a directed graph) describing which degrees of freedom interact under the dynamics. we tackle the problem of learning such a network from observation of the system trajectory over a time interval t. we analyse the l1-regularized least squares algorithm and, in the setting in which the underlying network is sparse, we prove performance guarantees that are uniform in the sampling rate as long as this is sufficiently high. this result substantiates the notion of a well defined ?time complexity? for the network inference problem.', metadata={'paper_id': 4055, 'year': 2010, 'authors': 'Jos? Pereira,Morteza Ibrahimi,Andrea Montanari', 'title': 'learning networks of stochastic differential equations'}),\n",
       "  Document(page_content='we consider the problem of learning, from k input data, a regression function in a function space of high dimension n using projections onto a random subspace of lower dimension m. from any linear approximation algorithm using empirical risk minimization (possibly penalized), we provide bounds on the excess risk of the estimate computed in the projected subspace (compressed domain) in terms of the excess risk of the estimate built in the high-dimensional space (initial domain). we apply the analysis to the ordinary least-squares regression and show that by choosing m=o(\\\\sqrt{k}), the estimation error (for the quadratic loss) of the ``compressed least squares regression is o(1/\\\\sqrt{k}) up to logarithmic factors. we also discuss the numerical complexity of several algorithms (both in initial and compressed domains) as a function of n, k, and m.', metadata={'paper_id': 3698, 'year': 2009, 'authors': 'Odalric Maillard,R?mi Munos', 'title': 'compressed least-squares regression'}),\n",
       "  Document(page_content=\"in the high-dimensional regression model a response variable is linearly related to $p$ covariates, but the sample size $n$ is smaller than $p$. we assume that only a small subset of covariates is `active' (i.e., the corresponding coefficients are non-zero), and consider the model-selection problem of identifying the active covariates. a popular approach is to estimate the regression coefficients through the lasso ($\\\\ell_1$-regularized least squares). this is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called `irrepresentability' condition. in this paper we study the `gauss-lasso' selector, a simple two-stage method that first solves the lasso, and then performs ordinary least squares restricted to the lasso active set.  we formulate `generalized irrepresentability condition' (gic), an assumption that is substantially weaker than irrepresentability. we prove that, under gic, the gauss-lasso correctly recovers the active set.\", metadata={'paper_id': 4930, 'year': 2013, 'authors': 'Adel Javanmard,Andrea Montanari', 'title': 'model selection for high-dimensional regression under the generalized irrepresentability condition'}),\n",
       "  Document(page_content='learning problems such as logistic regression are typically formulated as pure optimization problems defined on some loss function. we argue that this view ignores the fact that the loss function depends on stochastically generated data which in turn determines an intrinsic scale of precision for statistical estimation. by considering the statistical properties of the update variables used during the optimization (e.g. gradients), we can construct frequentist hypothesis tests to determine the reliability of these updates. we utilize subsets of the data for computing updates, and use the hypothesis tests for determining when the batch-size needs to be increased. this provides computational benefits and avoids overfitting by stopping when the batch-size has become equal to size of the full dataset. moreover, the proposed algorithms depend on a single interpretable parameter ? the probability for an update to be in the wrong direction ? which is set to a single value across all algorithms and datasets. in this paper, we illustrate these ideas on three l1 regularized coordinate algorithms: l1 -regularized l2 -loss svms, l1 -regularized logistic regression, and the lasso, but we emphasize that the underlying methods are much more generally applicable.', metadata={'paper_id': 4308, 'year': 2011, 'authors': 'Levi Boyles,Anoop Korattikara,Deva Ramanan,Max Welling', 'title': 'statistical tests for optimization efficiency'}),\n",
       "  Document(page_content='we consider logistic regression with arbitrary outliers in the covariate matrix. we propose a new robust logistic regression algorithm, called rolr, that estimates the parameter through a simple linear programming procedure. we prove that rolr is robust to a constant fraction of adversarial outliers. to the best of our knowledge, this is the first result on estimating logistic regression model when the covariate matrix is corrupted with any performance guarantees. besides regression, we apply rolr to solving binary classification problems where a fraction of training samples are corrupted.', metadata={'paper_id': 5515, 'year': 2014, 'authors': 'Jiashi Feng,Huan Xu,Shie Mannor,Shuicheng Yan', 'title': 'robust logistic regression and classification'}),\n",
       "  Document(page_content='linear regression models have been successfully used to function estimation and model selection in high-dimensional data analysis. however, most existing methods are built on least squares with the mean square error (mse) criterion, which are sensitive to outliers and their performance may be degraded for heavy-tailed noise. in this paper, we go beyond this criterion by investigating the regularized modal regression from a statistical learning viewpoint. a new regularized modal regression model is proposed for estimation and variable selection, which is robust to outliers, heavy-tailed noise, and skewed noise. on the theoretical side, we establish the approximation estimate for learning the conditional mode function, the sparsity analysis for variable selection, and the robustness characterization. on the application side, we applied our model to successfully improve the cognitive impairment prediction using the alzheimer?s disease neuroimaging initiative (adni) cohort data.', metadata={'paper_id': 6743, 'year': 2017, 'authors': 'Xiaoqian Wang,Hong Chen,Weidong Cai,Dinggang Shen,Heng Huang', 'title': 'regularized modal regression with applications in cognitive impairment prediction'}),\n",
       "  Document(page_content='learning a regression function using censored or interval-valued output data is an important problem in fields such as genomics and medicine. the goal is to learn a real-valued prediction function, and the training output labels indicate an interval of possible values. whereas most existing algorithms for this task are linear models, in this paper we investigate learning nonlinear tree models. we propose to learn a tree by minimizing a margin-based discriminative objective function, and we provide a dynamic programming algorithm for computing the optimal solution in log-linear time. we show empirically that this algorithm achieves state-of-the-art speed and prediction accuracy in a benchmark of several data sets.', metadata={'paper_id': 7080, 'year': 2017, 'authors': 'Alexandre Drouin,Toby Hocking,Francois Laviolette', 'title': 'maximum margin interval trees'}),\n",
       "  Document(page_content='this paper proposes an efficient algorithm (holrr) to handle regression tasks where the outputs have a tensor structure. we formulate the regression problem as the minimization of a least square criterion under a multilinear rank constraint, a difficult non convex problem. holrr computes efficiently an approximate solution of this problem, with solid theoretical guarantees. a kernel extension is also presented. experiments on synthetic and real data show that holrr computes accurate solutions while being computationally very competitive.', metadata={'paper_id': 6302, 'year': 2016, 'authors': 'Guillaume Rabusseau,Hachem Kadri', 'title': 'low-rank regression with tensor responses'}),\n",
       "  Document(page_content='this paper proposes an efficient online learning algorithm to track the smoothing functions of additive models. the key idea is to combine the linear representation of additive models with a recursive least squares (rls) filter. in order to quickly track changes in the model and put more weight on recent data, the rls filter uses a forgetting factor which exponentially weights down observations by the order of their arrival. the tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors. using results from lyapunov stability theory, upper bounds for the learning rate are analyzed. the proposed algorithm is applied to 5 years of electricity load data provided by the french utility company electricite de france (edf). compared to state-of-the-art methods, it achieves a superior performance in terms of model tracking and prediction accuracy.', metadata={'paper_id': 4724, 'year': 2012, 'authors': 'Amadou Ba,Mathieu Sinn,Yannig Goude,Pascal Pompey', 'title': 'adaptive learning of smoothing functions: application to electricity load forecasting'}),\n",
       "  Document(page_content='accounts of how people learn functional relationships between continuous variables have tended to focus on two possibilities: that people are estimating explicit functions, or that they are simply performing associative learning supported by similarity. we provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics. using the equivalence of bayesian linear regression and gaussian processes, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem. we use this insight to define a gaussian process model of human function learning that combines the strengths of both approaches.', metadata={'paper_id': 3529, 'year': 2008, 'authors': 'Thomas L. Griffiths,Chris Lucas,Joseph Williams,Michael L. Kalish', 'title': 'modeling human function learning with gaussian processes'}),\n",
       "  Document(page_content='this paper tackles the problem of selecting among several linear estimators in non-parametric regression; this includes model selection for linear regression, the choice of a regularization parameter in kernel ridge regression or spline smoothing, and the choice of a kernel in multiple kernel learning. we propose a new algorithm which first estimates consistently the variance of the noise, based upon the concept of minimal penalty which was previously introduced in the context of model selection. then, plugging our variance estimate in mallows $c_l$ penalty is proved to lead to an algorithm satisfying an oracle inequality. simulation experiments with kernel ridge regression and multiple kernel learning show that the proposed algorithm often improves significantly existing calibration procedures such as 10-fold cross-validation or generalized cross-validation.', metadata={'paper_id': 3639, 'year': 2009, 'authors': 'Sylvain Arlot,Francis R. Bach', 'title': 'data-driven calibration of linear estimators with minimal penalties'}),\n",
       "  Document(page_content='we consider the minimization of a convex objective function defined on a hilbert space, which is only available through unbiased estimates of its gradients. this problem includes standard machine learning algorithms such as kernel logistic regression and least-squares regression, and is commonly referred to as a stochastic approximation problem in the operations research community. we provide a non-asymptotic analysis of the convergence of two well-known algorithms, stochastic gradient descent (a.k.a.~robbins-monro algorithm) as well as a simple modification where iterates are averaged (a.k.a.~polyak-ruppert averaging). our analysis suggests that a learning rate proportional to the inverse of the number of iterations, while leading to the optimal convergence rate in the strongly convex case, is not robust to the lack of strong convexity or the setting of the proportionality constant. this situation is remedied when using slower decays together with averaging, robustly leading to the optimal rate of convergence. we illustrate our theoretical results with simulations on synthetic and standard datasets.', metadata={'paper_id': 4316, 'year': 2011, 'authors': 'Eric Moulines,Francis R. Bach', 'title': 'non-asymptotic analysis of stochastic approximation algorithms for machine learning'})]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## WARNING this cell may overshoot your quesry costs\n",
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb_gpt.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 20}),\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"map_reduce\"\n",
    ")\n",
    "\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "54827ce2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:30.135833Z",
     "start_time": "2023-08-02T18:02:23.826454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is linear regression?',\n",
       " 'result': '\\n\\nLinear regression is a statistical technique used to model the relationship between a dependent variable (the response variable) and one or more independent variables (the features). It is used to predict the value of the response variable based on the values of the features. It is typically used to guide decisions by estimating regression coefficients via ordinary least squares and using them to make decisions. Additionally, linear regression studies the problem of estimating a model parameter $\\\\beta^* \\\\in \\\\r^p$, from $n$ observations $\\\\{(y_i,x_i)\\\\}_{i=1}^n$ from linear model $y_i = \\\\langle \\\\x_i,\\\\beta^* \\\\rangle + \\\\epsilon_i$. It also considers a significant generalization in log-linear models, which are widely used probability models for statistical pattern recognition. These models are typically trained according to a convex criterion, and the optimization of log-linear model parameters is costly and therefore an important topic, in particular for large-scale applications. Different optimization algorithms have been evaluated empirically in many papers, and by making use of the convergence analysis, good results can be obtained on large-scale continuous handwriting recognition tasks with a simple and generic approach. Additionally',\n",
       " 'source_documents': [Document(page_content='when used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. when there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients. empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. we propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization. we demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. we also develop a theory that motivates the algorithm.', metadata={'paper_id': 3686, 'year': 2009, 'authors': 'Yi-hao Kao,Benjamin V. Roy,Xiang Yan', 'title': 'directed regression'}),\n",
       "  Document(page_content='this article considers algorithmic and statistical aspects of linear regression when the correspondence between the covariates and the responses is unknown. first, a fully polynomial-time approximation scheme is given for the natural least squares optimization problem in any constant dimension. next, in an average-case and noise-free setting where the responses exactly correspond to a linear function of i.i.d. draws from a standard multivariate normal distribution, an efficient algorithm based on lattice basis reduction is shown to exactly recover the unknown linear function in arbitrary dimension. finally, lower bounds on the signal-to-noise ratio are established for approximate recovery of the unknown linear function by any estimator.', metadata={'paper_id': 6751, 'year': 2017, 'authors': 'Daniel J. Hsu,Kevin Shi,Xiaorui Sun', 'title': 'linear regression without correspondence'}),\n",
       "  Document(page_content='linear regression studies the problem of estimating a model parameter $\\\\beta^* \\\\in \\\\r^p$, from $n$ observations $\\\\{(y_i,x_i)\\\\}_{i=1}^n$ from linear model $y_i = \\\\langle \\\\x_i,\\\\beta^* \\\\rangle + \\\\epsilon_i$. we consider a significant generalization in which the relationship between $\\\\langle x_i,\\\\beta^* \\\\rangle$ and $y_i$ is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as well as unknown. this model is known as the single-index model in statistics, and, among other things, it represents a significant generalization of one-bit compressed sensing. we propose a novel spectral-based estimation procedure and show that we can recover $\\\\beta^*$ in settings (i.e., classes of link function $f$) where previous algorithms fail. in general, our algorithm requires only very mild restrictions on the (unknown) functional relationship between $y_i$ and $\\\\langle x_i,\\\\beta^* \\\\rangle$. we also consider the high dimensional setting where $\\\\beta^*$ is sparse, and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where $p \\\\gg n$. for a broad class of link functions between $\\\\langle x_i,\\\\beta^* \\\\rangle$ and $y_i$, we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes.', metadata={'paper_id': 6013, 'year': 2015, 'authors': 'Xinyang Yi,Zhaoran Wang,Constantine Caramanis,Han Liu', 'title': 'optimal linear estimation under unknown nonlinear transform'}),\n",
       "  Document(page_content='in a regression task, a predictor is given a set of instances, along with a real value for each point. subsequently, she has to identify the value of a new instance as accurately as possible. in this work, we initiate the study of strategic predictions in machine learning. we consider a regression task tackled by two players, where the payoff of each player is the proportion of the points she predicts more accurately than the other player. we first revise the probably approximately correct learning framework to deal with the case of a duel between two predictors. we then devise an algorithm which finds a linear regression predictor that is a best response to any (not necessarily linear) regression algorithm. we show that it has linearithmic sample complexity, and polynomial time complexity when the dimension of the instances domain is fixed. we also test our approach in a high-dimensional setting, and show it significantly defeats classical regression algorithms in the prediction duel. together, our work introduces a novel machine learning task that lends itself well to current competitive online settings, provides its theoretical foundations, and illustrates its applicability.', metadata={'paper_id': 6748, 'year': 2017, 'authors': 'Omer Ben Porat,Moshe Tennenholtz', 'title': 'best response regression'}),\n",
       "  Document(page_content='online sparse linear regression is the task of applying linear regression analysis to examples arriving sequentially subject to a resource constraint that a limited number of features of examples can be observed. despite its importance in many practical applications, it has been recently shown that there is no polynomial-time sublinear-regret algorithm unless np$\\\\subseteq$bpp, and only an exponential-time sublinear-regret algorithm has been found. in this paper, we introduce mild assumptions to solve the problem. under these assumptions, we present polynomial-time sublinear-regret algorithms for the online sparse linear regression. in addition, thorough experiments with publicly available data demonstrate that our algorithms outperform other known algorithms.', metadata={'paper_id': 6998, 'year': 2017, 'authors': 'Shinji Ito,Daisuke Hatano,Hanna Sumita,Akihiro Yabe,Takuro Fukunaga,Naonori Kakimura,Ken-Ichi Kawarabayashi', 'title': 'efficient sublinear-regret algorithms for online sparse linear regression with limited observation'}),\n",
       "  Document(page_content='log-linear models are widely used probability models for statistical pattern recognition. typically, log-linear models are trained according to a convex criterion. in recent years, the interest in log-linear models has greatly increased. the optimization of log-linear model parameters is costly and therefore an important topic, in particular for large-scale applications. different optimization algorithms have been evaluated empirically in many papers. in this work, we analyze the optimization problem analytically and show that the training of log-linear models can be highly ill-conditioned. we verify our findings on two handwriting tasks. by making use of our convergence analysis, we obtain good results on a large-scale continuous handwriting recognition task with a simple and generic approach.', metadata={'paper_id': 4421, 'year': 2011, 'authors': 'Simon Wiesler,Hermann Ney', 'title': 'a convergence analysis of log-linear training'}),\n",
       "  Document(page_content='we study regression and classification in a setting where the learning algorithm is allowed to access only a limited number of attributes per example, known as the limited attribute observation model. in this well-studied model, we provide the first lower bounds giving a limit on the precision attainable by any algorithm for several variants of regression, notably linear regression with the absolute loss and the squared loss, as well as for classification with the hinge loss. we complement these lower bounds with a general purpose algorithm that gives an upper bound on the achievable precision limit in the setting of learning with missing data.', metadata={'paper_id': 6171, 'year': 2016, 'authors': 'Brian Bullins,Elad Hazan,Tomer Koren', 'title': 'the limits of learning with missing data'}),\n",
       "  Document(page_content='we present a new empirical risk minimization framework for approximating functions from training samples for low-dimensional regression applications where a lattice (look-up table) is stored and interpolated at run-time for an efficient hardware implementation. rather than evaluating a fitted function at the lattice nodes without regard to the fact that samples will be interpolated, the proposed lattice regression approach estimates the lattice to minimize the interpolation error on the given training samples. experiments show that lattice regression can reduce mean test error compared to gaussian process regression for digital color management of printers, an application for which linearly interpolating a look-up table (lut) is standard. simulations confirm that lattice regression performs consistently better than the naive approach to learning the lattice, particularly when the density of training samples is low.', metadata={'paper_id': 3694, 'year': 2009, 'authors': 'Eric Garcia,Maya Gupta', 'title': 'lattice regression'}),\n",
       "  Document(page_content='we consider linear models for stochastic dynamics. any such model can be associated a network (namely a directed graph) describing which degrees of freedom interact under the dynamics. we tackle the problem of learning such a network from observation of the system trajectory over a time interval t. we analyse the l1-regularized least squares algorithm and, in the setting in which the underlying network is sparse, we prove performance guarantees that are uniform in the sampling rate as long as this is sufficiently high. this result substantiates the notion of a well defined ?time complexity? for the network inference problem.', metadata={'paper_id': 4055, 'year': 2010, 'authors': 'Jos? Pereira,Morteza Ibrahimi,Andrea Montanari', 'title': 'learning networks of stochastic differential equations'}),\n",
       "  Document(page_content='we consider the problem of learning, from k input data, a regression function in a function space of high dimension n using projections onto a random subspace of lower dimension m. from any linear approximation algorithm using empirical risk minimization (possibly penalized), we provide bounds on the excess risk of the estimate computed in the projected subspace (compressed domain) in terms of the excess risk of the estimate built in the high-dimensional space (initial domain). we apply the analysis to the ordinary least-squares regression and show that by choosing m=o(\\\\sqrt{k}), the estimation error (for the quadratic loss) of the ``compressed least squares regression is o(1/\\\\sqrt{k}) up to logarithmic factors. we also discuss the numerical complexity of several algorithms (both in initial and compressed domains) as a function of n, k, and m.', metadata={'paper_id': 3698, 'year': 2009, 'authors': 'Odalric Maillard,R?mi Munos', 'title': 'compressed least-squares regression'}),\n",
       "  Document(page_content=\"in the high-dimensional regression model a response variable is linearly related to $p$ covariates, but the sample size $n$ is smaller than $p$. we assume that only a small subset of covariates is `active' (i.e., the corresponding coefficients are non-zero), and consider the model-selection problem of identifying the active covariates. a popular approach is to estimate the regression coefficients through the lasso ($\\\\ell_1$-regularized least squares). this is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called `irrepresentability' condition. in this paper we study the `gauss-lasso' selector, a simple two-stage method that first solves the lasso, and then performs ordinary least squares restricted to the lasso active set.  we formulate `generalized irrepresentability condition' (gic), an assumption that is substantially weaker than irrepresentability. we prove that, under gic, the gauss-lasso correctly recovers the active set.\", metadata={'paper_id': 4930, 'year': 2013, 'authors': 'Adel Javanmard,Andrea Montanari', 'title': 'model selection for high-dimensional regression under the generalized irrepresentability condition'}),\n",
       "  Document(page_content='learning problems such as logistic regression are typically formulated as pure optimization problems defined on some loss function. we argue that this view ignores the fact that the loss function depends on stochastically generated data which in turn determines an intrinsic scale of precision for statistical estimation. by considering the statistical properties of the update variables used during the optimization (e.g. gradients), we can construct frequentist hypothesis tests to determine the reliability of these updates. we utilize subsets of the data for computing updates, and use the hypothesis tests for determining when the batch-size needs to be increased. this provides computational benefits and avoids overfitting by stopping when the batch-size has become equal to size of the full dataset. moreover, the proposed algorithms depend on a single interpretable parameter ? the probability for an update to be in the wrong direction ? which is set to a single value across all algorithms and datasets. in this paper, we illustrate these ideas on three l1 regularized coordinate algorithms: l1 -regularized l2 -loss svms, l1 -regularized logistic regression, and the lasso, but we emphasize that the underlying methods are much more generally applicable.', metadata={'paper_id': 4308, 'year': 2011, 'authors': 'Levi Boyles,Anoop Korattikara,Deva Ramanan,Max Welling', 'title': 'statistical tests for optimization efficiency'}),\n",
       "  Document(page_content='we consider logistic regression with arbitrary outliers in the covariate matrix. we propose a new robust logistic regression algorithm, called rolr, that estimates the parameter through a simple linear programming procedure. we prove that rolr is robust to a constant fraction of adversarial outliers. to the best of our knowledge, this is the first result on estimating logistic regression model when the covariate matrix is corrupted with any performance guarantees. besides regression, we apply rolr to solving binary classification problems where a fraction of training samples are corrupted.', metadata={'paper_id': 5515, 'year': 2014, 'authors': 'Jiashi Feng,Huan Xu,Shie Mannor,Shuicheng Yan', 'title': 'robust logistic regression and classification'}),\n",
       "  Document(page_content='linear regression models have been successfully used to function estimation and model selection in high-dimensional data analysis. however, most existing methods are built on least squares with the mean square error (mse) criterion, which are sensitive to outliers and their performance may be degraded for heavy-tailed noise. in this paper, we go beyond this criterion by investigating the regularized modal regression from a statistical learning viewpoint. a new regularized modal regression model is proposed for estimation and variable selection, which is robust to outliers, heavy-tailed noise, and skewed noise. on the theoretical side, we establish the approximation estimate for learning the conditional mode function, the sparsity analysis for variable selection, and the robustness characterization. on the application side, we applied our model to successfully improve the cognitive impairment prediction using the alzheimer?s disease neuroimaging initiative (adni) cohort data.', metadata={'paper_id': 6743, 'year': 2017, 'authors': 'Xiaoqian Wang,Hong Chen,Weidong Cai,Dinggang Shen,Heng Huang', 'title': 'regularized modal regression with applications in cognitive impairment prediction'}),\n",
       "  Document(page_content='learning a regression function using censored or interval-valued output data is an important problem in fields such as genomics and medicine. the goal is to learn a real-valued prediction function, and the training output labels indicate an interval of possible values. whereas most existing algorithms for this task are linear models, in this paper we investigate learning nonlinear tree models. we propose to learn a tree by minimizing a margin-based discriminative objective function, and we provide a dynamic programming algorithm for computing the optimal solution in log-linear time. we show empirically that this algorithm achieves state-of-the-art speed and prediction accuracy in a benchmark of several data sets.', metadata={'paper_id': 7080, 'year': 2017, 'authors': 'Alexandre Drouin,Toby Hocking,Francois Laviolette', 'title': 'maximum margin interval trees'}),\n",
       "  Document(page_content='this paper proposes an efficient algorithm (holrr) to handle regression tasks where the outputs have a tensor structure. we formulate the regression problem as the minimization of a least square criterion under a multilinear rank constraint, a difficult non convex problem. holrr computes efficiently an approximate solution of this problem, with solid theoretical guarantees. a kernel extension is also presented. experiments on synthetic and real data show that holrr computes accurate solutions while being computationally very competitive.', metadata={'paper_id': 6302, 'year': 2016, 'authors': 'Guillaume Rabusseau,Hachem Kadri', 'title': 'low-rank regression with tensor responses'}),\n",
       "  Document(page_content='this paper proposes an efficient online learning algorithm to track the smoothing functions of additive models. the key idea is to combine the linear representation of additive models with a recursive least squares (rls) filter. in order to quickly track changes in the model and put more weight on recent data, the rls filter uses a forgetting factor which exponentially weights down observations by the order of their arrival. the tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors. using results from lyapunov stability theory, upper bounds for the learning rate are analyzed. the proposed algorithm is applied to 5 years of electricity load data provided by the french utility company electricite de france (edf). compared to state-of-the-art methods, it achieves a superior performance in terms of model tracking and prediction accuracy.', metadata={'paper_id': 4724, 'year': 2012, 'authors': 'Amadou Ba,Mathieu Sinn,Yannig Goude,Pascal Pompey', 'title': 'adaptive learning of smoothing functions: application to electricity load forecasting'}),\n",
       "  Document(page_content='accounts of how people learn functional relationships between continuous variables have tended to focus on two possibilities: that people are estimating explicit functions, or that they are simply performing associative learning supported by similarity. we provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics. using the equivalence of bayesian linear regression and gaussian processes, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem. we use this insight to define a gaussian process model of human function learning that combines the strengths of both approaches.', metadata={'paper_id': 3529, 'year': 2008, 'authors': 'Thomas L. Griffiths,Chris Lucas,Joseph Williams,Michael L. Kalish', 'title': 'modeling human function learning with gaussian processes'}),\n",
       "  Document(page_content='this paper tackles the problem of selecting among several linear estimators in non-parametric regression; this includes model selection for linear regression, the choice of a regularization parameter in kernel ridge regression or spline smoothing, and the choice of a kernel in multiple kernel learning. we propose a new algorithm which first estimates consistently the variance of the noise, based upon the concept of minimal penalty which was previously introduced in the context of model selection. then, plugging our variance estimate in mallows $c_l$ penalty is proved to lead to an algorithm satisfying an oracle inequality. simulation experiments with kernel ridge regression and multiple kernel learning show that the proposed algorithm often improves significantly existing calibration procedures such as 10-fold cross-validation or generalized cross-validation.', metadata={'paper_id': 3639, 'year': 2009, 'authors': 'Sylvain Arlot,Francis R. Bach', 'title': 'data-driven calibration of linear estimators with minimal penalties'}),\n",
       "  Document(page_content='we consider the minimization of a convex objective function defined on a hilbert space, which is only available through unbiased estimates of its gradients. this problem includes standard machine learning algorithms such as kernel logistic regression and least-squares regression, and is commonly referred to as a stochastic approximation problem in the operations research community. we provide a non-asymptotic analysis of the convergence of two well-known algorithms, stochastic gradient descent (a.k.a.~robbins-monro algorithm) as well as a simple modification where iterates are averaged (a.k.a.~polyak-ruppert averaging). our analysis suggests that a learning rate proportional to the inverse of the number of iterations, while leading to the optimal convergence rate in the strongly convex case, is not robust to the lack of strong convexity or the setting of the proportionality constant. this situation is remedied when using slower decays together with averaging, robustly leading to the optimal rate of convergence. we illustrate our theoretical results with simulations on synthetic and standard datasets.', metadata={'paper_id': 4316, 'year': 2011, 'authors': 'Eric Moulines,Francis R. Bach', 'title': 'non-asymptotic analysis of stochastic approximation algorithms for machine learning'})]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## WARNING this cell may overshoot your quesry costs\n",
    "qa_chain_refine = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb_gpt.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 20}),\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"refine\"\n",
    ")\n",
    "\n",
    "result = qa_chain_refine({\"query\": question})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "51923140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:30.147257Z",
     "start_time": "2023-08-02T18:06:30.139753Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "def query_db(db, users_question, llm, k=10,filter={}):\n",
    "  # define the prompt template\n",
    "  template = \"\"\"\n",
    "  Given the following context sections, answer the\n",
    "  question using only the given context. If you are unsure and the answer is not\n",
    "  explicitly writting in the documentation, say \"Sorry, I don't know how to help with that.\"\n",
    "\n",
    "  Context sections:\n",
    "  {context}\n",
    "\n",
    "  Question:\n",
    "  {users_question}\n",
    "\n",
    "  Answer:\n",
    "  \"\"\"\n",
    "  prompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])\n",
    "    # use our vector store to find similar text chunks\n",
    "  results = db.similarity_search(\n",
    "      query=users_question,\n",
    "      n_results=k,\n",
    "      filter=filter\n",
    "  )\n",
    "\n",
    "\n",
    "  # fill the prompt template\n",
    "  prompt_text = prompt.format(context = results, users_question = users_question)\n",
    "\n",
    "  # ask the defined LLM\n",
    "  return llm(prompt_text)\n",
    "\n",
    "\n",
    "def query_db_relevance(db, users_question, llm, k=10, filter={}):\n",
    "  # define the prompt template\n",
    "  template = \"\"\"\n",
    "  Given the following context sections, answer the\n",
    "  question using only the given context. If you are unsure and the answer is not\n",
    "  explicitly writting in the documentation, say \"Sorry, I don't know how to help with that.\"\n",
    "\n",
    "  Context sections:\n",
    "  {context}\n",
    "\n",
    "  Question:\n",
    "  {users_question}\n",
    "\n",
    "  Answer:\n",
    "  \"\"\"\n",
    "  prompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])\n",
    "    # use our vector store to find similar text chunks\n",
    "  results = db.max_marginal_relevance_search(\n",
    "      query=users_question,\n",
    "      n_results=k,\n",
    "      filter=filter\n",
    "  )\n",
    "\n",
    "  # fill the prompt template\n",
    "  prompt_text = prompt.format(context = results, users_question = users_question)\n",
    "\n",
    "  # ask the defined LLM\n",
    "  return llm(prompt_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "76d64bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:34.102915Z",
     "start_time": "2023-08-02T18:06:30.150225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linear regression is a method of estimating a model parameter from observations from a linear model, where the relationship between the covariates and the responses is unknown. It typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(vectordb_gpt,\"What is linear regression?\" , llm, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a8f5cadb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:37.449600Z",
     "start_time": "2023-08-02T18:06:34.108531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Linear regression is a method of estimating a model parameter from observations of a linear model, where the relationship between the model parameter and the observations is noisy, quantized to a single bit, potentially nonlinear, noninvertible, and unknown.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db_relevance(vectordb_gpt,\"What is linear regression?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "94573f65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:40.240438Z",
     "start_time": "2023-08-02T18:06:37.471421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linear regression is a method of estimating a model parameter from observations from a linear model, where the relationship between the covariates and the responses is unknown. It typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(elasticdb_gpt,\"What is linear regression?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "787663b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:42.984086Z",
     "start_time": "2023-08-02T18:06:40.242668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. It is used to build systems that can understand, interpret, and manipulate human language in order to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(vectordb_gpt,\"What is Natural language processing?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "87c172fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:46.440869Z",
     "start_time": "2023-08-02T18:06:42.987772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. It is used to build systems that can understand, interpret, and manipulate human language in order to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(elasticdb_gpt,\"What is Natural language processing?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b8d6df13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:51.316165Z",
     "start_time": "2023-08-02T18:06:46.444820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. It is used to build systems that can understand, interpret, and manipulate human language in order to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(vectordb_gpt,\"What is Natural language processing?\" , llm, {\"year\":1990})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "92850303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:53.745753Z",
     "start_time": "2023-08-02T18:06:51.318773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. It is used to build systems that can understand, interpret, and manipulate human language, including speech recognition, natural language understanding, and natural language generation.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(elasticdb_gpt,\"What is Natural language processing?\" , llm, {\"year\":1990})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7c440512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:54.885101Z",
     "start_time": "2023-08-02T18:06:53.749420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry, I don't know how to help with that.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(vectordb_gpt,\"What is BERT?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c004813d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:56.207572Z",
     "start_time": "2023-08-02T18:06:54.888259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry, I don't know how to help with that.\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db_relevance(vectordb_gpt,\"What is BERT?\" , llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4a56c",
   "metadata": {},
   "source": [
    "## Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8d871770",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:56.224890Z",
     "start_time": "2023-08-02T18:06:56.211223Z"
    }
   },
   "outputs": [],
   "source": [
    "new_papers = [{\n",
    "    \"title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n",
    "    \"authors\": \"Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova\",\n",
    "    \"year\": 2018,\n",
    "    \"paper_id\": 7301,\n",
    "    \"abstract\": \"\"\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
    "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
    "\"\"\"\n",
    "},\n",
    "  {\n",
    "      \"title\" : \"Evolution of transfer learning in natural language processing\",\n",
    "       \"authors\": \"Aditya Malte, Pratik Ratadiya\",\n",
    "      \"year\": 2019,\n",
    "       \"paper_id\": 7302,\n",
    "      \"abstract\": \"\"\"In this paper, we present a study of the recent advancements which have helped bring Transfer Learning to NLP through the use of semi-supervised training. We discuss cutting-edge methods and architectures such as BERT, GPT, ELMo, ULMFit among others. Classically, tasks in natural language processing have been performed through rule-based and statistical methodologies. However, owing to the vast nature of natural languages these methods do not generalise well and failed to learn the nuances of language. Thus machine learning algorithms such as Naive Bayes and decision trees coupled with traditional models such as Bag-of-Words and N-grams were used to usurp this problem. Eventually, with the advent of advanced recurrent neural network architectures such as the LSTM, we were able to achieve state-of-the-art performance in several natural language processing tasks such as text classification and machine translation. We talk about how Transfer Learning has brought about the well-known ImageNet moment for NLP. Several advanced architectures such as the Transformer and its variants have allowed practitioners to leverage knowledge gained from unrelated task to drastically fasten convergence and provide better performance on the target task. This survey represents an effort at providing a succinct yet complete understanding of the recent advances in natural language processing using deep learning in with a special focus on detailing transfer learning and its potential advantages.\n",
    "\"\"\"    \n",
    "  },\n",
    "  {\n",
    "       \"title\" : \"BERTQA -- Attention on Steroids\",\n",
    "       \"authors\": \"Ankit Chadha, Rewa Sood\",\n",
    "      \"year\": 2019,\n",
    "        \"paper_id\": 7303,\n",
    "      \"abstract\": \"\"\"In this work, we extend the Bidirectional Encoder Representations from Transformers (BERT) with an emphasis on directed coattention to obtain an improved F1 performance on the SQUAD2.0 dataset. The Transformer architecture on which BERT is based places hierarchical global attention on the concatenation of the context and query. Our additions to the BERT architecture augment this attention with a more focused context to query (C2Q) and query to context (Q2C) attention via a set of modified Transformer encoder units. In addition, we explore adding convolution-based feature extraction within the coattention architecture to add localized information to self-attention. We found that coattention significantly improves the no answer F1 by 4 points in the base and 1 point in the large architecture. After adding skip connections the no answer F1 improved further without causing an additional loss in has answer F1. The addition of localized feature extraction added to attention produced an overall dev F1 of 77.03 in the base architecture. We applied our findings to the large BERT model which contains twice as many layers and further used our own augmented version of the SQUAD 2.0 dataset created by back translation, which we have named SQUAD 2.Q. Finally, we performed hyperparameter tuning and ensembled our best models for a final F1/EM of 82.317/79.442 (Attention on Steroids, PCE Test Leaderboard).\n",
    "\"\"\"             \n",
    "  },\n",
    "    {\n",
    "        \"title\": \"BERT: A Review of Applications in Natural Language Processing and Understanding\",\n",
    "        \"authors\": \"Mikhail Koroteev\",\n",
    "        \"year\": 2021,\n",
    "        \"paper_id\": 7304,\n",
    "        \"abstract\": \"In this review, we describe the application of one of the most popular deep learning-based language models - BERT. The paper describes the mechanism of operation of this model, the main areas of its application to the tasks of text analytics, comparisons with similar models in each task, as well as a description of some proprietary models. In preparing this review, the data of several dozen original scientific articles published over the past few years, which attracted the most attention in the scientific community, were systematized. This survey will be useful to all students and researchers who want to get acquainted with the latest advances in the field of natural language text analysis.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0981f3de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:56.233848Z",
     "start_time": "2023-08-02T18:06:56.228157Z"
    }
   },
   "outputs": [],
   "source": [
    "new_paper_df = pd.DataFrame(new_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "be2f0e47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:56.339717Z",
     "start_time": "2023-08-02T18:06:56.237238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kris...</td>\n",
       "      <td>2018</td>\n",
       "      <td>7301</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Evolution of transfer learning in natural lang...</td>\n",
       "      <td>Aditya Malte, Pratik Ratadiya</td>\n",
       "      <td>2019</td>\n",
       "      <td>7302</td>\n",
       "      <td>In this paper, we present a study of the recen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BERTQA -- Attention on Steroids</td>\n",
       "      <td>Ankit Chadha, Rewa Sood</td>\n",
       "      <td>2019</td>\n",
       "      <td>7303</td>\n",
       "      <td>In this work, we extend the Bidirectional Enco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BERT: A Review of Applications in Natural Lang...</td>\n",
       "      <td>Mikhail Koroteev</td>\n",
       "      <td>2021</td>\n",
       "      <td>7304</td>\n",
       "      <td>In this review, we describe the application of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  BERT: Pre-training of Deep Bidirectional Trans...   \n",
       "1  Evolution of transfer learning in natural lang...   \n",
       "2                    BERTQA -- Attention on Steroids   \n",
       "3  BERT: A Review of Applications in Natural Lang...   \n",
       "\n",
       "                                             authors  year  paper_id  \\\n",
       "0  Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kris...  2018      7301   \n",
       "1                      Aditya Malte, Pratik Ratadiya  2019      7302   \n",
       "2                            Ankit Chadha, Rewa Sood  2019      7303   \n",
       "3                                   Mikhail Koroteev  2021      7304   \n",
       "\n",
       "                                            abstract  \n",
       "0  We introduce a new language representation mod...  \n",
       "1  In this paper, we present a study of the recen...  \n",
       "2  In this work, we extend the Bidirectional Enco...  \n",
       "3  In this review, we describe the application of...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_paper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1dce5812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:56.360358Z",
     "start_time": "2023-08-02T18:06:56.355885Z"
    }
   },
   "outputs": [],
   "source": [
    "loader_new = DataFrameLoader(new_paper_df, page_content_column=\"abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f0a69bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:56.368455Z",
     "start_time": "2023-08-02T18:06:56.363830Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_splits = sentence_spltter.split_documents(loader_new.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f2cad741",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:06:57.895710Z",
     "start_time": "2023-08-02T18:06:56.373702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5dffc854-315f-11ee-acba-acde48001122',\n",
       " '5dffc9a8-315f-11ee-acba-acde48001122',\n",
       " '5dffca20-315f-11ee-acba-acde48001122',\n",
       " '5dffca7a-315f-11ee-acba-acde48001122']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_gpt.add_documents(loader_new.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3ee1e8f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:34:43.667195Z",
     "start_time": "2023-08-02T18:34:40.294701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ebb84620-de03-4c20-9535-29b26cf9f0a4',\n",
       " 'e15da2df-f4ca-45c6-af8d-bbe3c2504f86',\n",
       " '217b46f5-b846-48e0-8f41-8515914a6397',\n",
       " '5baf7b29-5fc1-45a0-bfb6-be6a9138cbe7']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticdb_gpt.add_documents(loader_new.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ffced776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:34:49.530678Z",
     "start_time": "2023-08-02T18:34:47.051719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BERT is a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(vectordb_gpt,\"What is BERT?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1cd77b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:34:52.347650Z",
     "start_time": "2023-08-02T18:34:49.536417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BERT is a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db_relevance(vectordb_gpt,\"What is BERT?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "68327365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:34:55.864591Z",
     "start_time": "2023-08-02T18:34:52.351945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BERT is a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db(elasticdb_gpt,\"What is BERT?\" , llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "40b596b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:10.129962Z",
     "start_time": "2023-08-02T18:34:55.870577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db_relevance(vectordb_gpt,\"How is bert an improvement?\" , llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac19231e",
   "metadata": {},
   "source": [
    "## Generating Questions for Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f277d422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:10.137514Z",
     "start_time": "2023-08-02T18:35:10.133457Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import QAGenerationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a7736d34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:10.145217Z",
     "start_time": "2023-08-02T18:35:10.139931Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import QAGenerationChain\n",
    "\n",
    "chain = QAGenerationChain.from_llm(ChatOpenAI(temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "20dc09c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:36.020318Z",
     "start_time": "2023-08-02T18:35:10.149715Z"
    }
   },
   "outputs": [],
   "source": [
    "questions = [chain.run(doc.page_content) for doc in docs[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a8e43877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:36.034991Z",
     "start_time": "2023-08-02T18:35:36.023437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'question': 'What is the purpose of non-negative matrix factorization (NMF)?',\n",
       "   'answer': 'NMF is a useful decomposition for multivariate data.'}],\n",
       " [{'question': 'What is the purpose of spike-triggered covariance method described in the text?',\n",
       "   'answer': 'The purpose of the spike-triggered covariance method is to retrieve suppressive components of the gain control signal in a neuron.'}],\n",
       " [{'question': 'What complexity class is the problem of determining whether a competitive posg has a positive-expected-reward strategy complete for?',\n",
       "   'answer': 'The problem is complete for the class nexp with an oracle for np.'}],\n",
       " [{'question': 'What is the main contribution of the presented algorithm?',\n",
       "   'answer': 'The main contribution of the presented algorithm is a polynomial algorithm for learning the structure of bounded-treewidth junction trees.'}],\n",
       " [{'question': 'What is the purpose of the local smoothness regularizer introduced in this paper?',\n",
       "   'answer': 'The purpose of the local smoothness regularizer is to improve the generalization and speed up the training of existing semi-supervised boosting algorithms.'}],\n",
       " [{'question': 'What is the purpose of the learning rule described in the text?',\n",
       "   'answer': 'The purpose of the learning rule is to optimize information bottleneck with spiking neurons.'}],\n",
       " [{'question': 'What did the study demonstrate about the combined model of face detection and low-level saliency?',\n",
       "   'answer': 'The study demonstrated that the combined model outperformed a low-level model in predicting locations humans fixate.'}],\n",
       " [{'question': 'What is the purpose of the infrastructure developed in this study?',\n",
       "   'answer': 'The purpose of the infrastructure is to automate the crawling, parsing, and storage of open source software code.'}],\n",
       " [{'question': 'What is the purpose of the model discussed in the text?',\n",
       "   'answer': 'The purpose of the model is to simulate the neural and hemodynamic activity underlying the observed blood oxygen level dependent (BOLD) signal in functional magnetic resonance imaging (fMRI).'}],\n",
       " [{'question': 'What is the purpose of independent component analysis (ICA)?',\n",
       "   'answer': 'The purpose of independent component analysis (ICA) is to decouple signals.'}]]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b785909",
   "metadata": {},
   "source": [
    "## Chatting with your data: Final Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4e8daff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:36.046366Z",
     "start_time": "2023-08-02T18:35:36.041143Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fbccf78d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:36.061428Z",
     "start_time": "2023-08-02T18:35:36.056198Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "retriever=vectordb_gpt.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e797ac09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:38.801233Z",
     "start_time": "2023-08-02T18:35:36.063923Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"What is Natural language processing?\"\n",
    "result = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2a6ffbf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:38.822200Z",
     "start_time": "2023-08-02T18:35:38.805143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. It is used to analyze, understand, and generate human language, including speech, text, and conversations.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fa5b9862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:41.663655Z",
     "start_time": "2023-08-02T18:35:38.827100Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"What are its real life applications?\"\n",
    "result2 = qa({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "38543a18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:41.678576Z",
     "start_time": "2023-08-02T18:35:41.667447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Natural Language Processing has many real-life applications, such as text classification, machine translation, question answering, dialog-based language learning, and reading comprehension.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " result2[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c0d9a92d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:41.688084Z",
     "start_time": "2023-08-02T18:35:41.682119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "current_date = datetime.datetime.now().date()\n",
    "if current_date < datetime.date(2023, 9, 2):\n",
    "    llm_name = \"gpt-3.5-turbo-0301\"\n",
    "else:\n",
    "    llm_name = \"gpt-3.5-turbo\"\n",
    "print(llm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "332c134b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:43.848912Z",
     "start_time": "2023-08-02T18:35:41.691101Z"
    }
   },
   "outputs": [],
   "source": [
    "import gradio\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history_ui\",\n",
    "    return_messages=True\n",
    ")\n",
    "retriever=vectordb_gpt.as_retriever()\n",
    "    \n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
    "        chain_type=\"stuff\", \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "def chat(message, history=None):\n",
    "    history = history or []\n",
    "    response = qa({\"question\": message, \"chat_history\":history})['answer']\n",
    "    history.append((message, response))\n",
    "    return history, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3d71bb01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T18:35:58.299502Z",
     "start_time": "2023-08-02T18:35:43.851711Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/q4kkg14x6c9bg29754hkb_yr0000gn/T/ipykernel_68400/3380930186.py:2: GradioDeprecationWarning: The 'color_map' parameter has been deprecated.\n",
      "  chatbot = gradio.Chatbot(color_map=(\"green\", \"gray\"))\n",
      "/var/folders/3c/q4kkg14x6c9bg29754hkb_yr0000gn/T/ipykernel_68400/3380930186.py:3: GradioDeprecationWarning: `allow_screenshot` parameter is deprecated, and it has no effect\n",
      "  interface = gradio.Interface(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://400293b200000a149b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://400293b200000a149b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collection.load()\n",
    "chatbot = gradio.Chatbot(color_map=(\"green\", \"gray\"))\n",
    "interface = gradio.Interface(\n",
    "    chat,\n",
    "    [\"text\", \"state\"],\n",
    "    [chatbot, \"state\"],\n",
    "    allow_screenshot=False,\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "interface.launch(inline=True, share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7054337",
   "metadata": {},
   "source": [
    "https://tinyurl.com/SemBot\n",
    "\n",
    "\n",
    "If you goto this url you can even try it on your own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DatahackSummitSemanticSearch",
   "language": "python",
   "name": "datahacksummitsemanticsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
